"""
notion_client.py - Notion API å®¢æˆ·ç«¯æ¨¡å—

è¯¥æ¨¡å—è´Ÿè´£ä¸Notion APIäº¤äº’ï¼Œåˆ›å»ºæ•°æ®åº“æ¡ç›®å’Œé¡µé¢ã€‚
éµå¾ªCleanRLè®¾è®¡åŸåˆ™ï¼šå•ä¸€èŒè´£ã€æ˜¾å¼ä¾èµ–ã€æ˜“äºæµ‹è¯•ã€‚
"""

import sys
import asyncio
import traceback
from datetime import datetime
from typing import List, Optional, Dict, Any

from notion_client import AsyncClient
from loguru import logger

from config import settings, PAPER_CATEGORIES
from models import (
    FullPaper,
    HFPaper,
    ClassificationResult,
    KeywordsResult,
    LabelsResult,
    CommentsResult,
    ParagraphComment,
)
from utils import format_exception, truncate_text


class NotionPaperClient:
    """
    Notionè®ºæ–‡ç®¡ç†å®¢æˆ·ç«¯
    
    è´Ÿè´£ï¼š
    1. åˆ›å»º/æ›´æ–°æ•°æ®åº“æ¡ç›®
    2. åˆ›å»ºè¯¦ç»†çš„è®ºæ–‡é¡µé¢
    3. ç¾è§‚çš„é¡µé¢å¸ƒå±€
    """
    
    def __init__(
        self,
        token: str = None,
        database_id: str = None,
    ):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            token: Notion API token
            database_id: æ•°æ®åº“ID
        """
        self.token = token or settings.notion_token
        self.database_id = database_id or settings.notion_database_id
        
        if not self.token:
            raise ValueError("Notion tokenæœªé…ç½®")
        if not self.database_id:
            raise ValueError("Notion database_idæœªé…ç½®")
        
        self.client = AsyncClient(auth=self.token)
        
        # ç»Ÿè®¡
        self.created_count = 0
        self.updated_count = 0
        self.error_count = 0
    
    async def check_connection(self) -> bool:
        """æ£€æŸ¥è¿æ¥"""
        try:
            await self.client.databases.retrieve(database_id=self.database_id)
            logger.info(f"Notionè¿æ¥æˆåŠŸ: {self.database_id[:8]}...")
            return True
        except Exception:
            error_message = format_exception()
            logger.error(f"Notionè¿æ¥å¤±è´¥: {error_message}")
            return False
    
    # ==================== æ•°æ®åº“æ“ä½œ ====================
    
    def _build_database_properties(self, paper: FullPaper) -> Dict[str, Any]:
        """æ„å»ºæ•°æ®åº“å±æ€§"""
        properties = {
            "Title": {
                "title": [{"text": {"content": truncate_text(paper.title, 100)}}]
            },
            "Paper ID": {
                "rich_text": [{"text": {"content": paper.paper_id}}]
            },
            "Authors": {
                "rich_text": [{"text": {"content": ", ".join(paper.authors[:5])}}]
            },
        }
        
        # åˆ†ç±»
        if paper.classification:
            properties["Category"] = {
                "select": {"name": paper.classification.category_name}
            }
        
        # å…³é”®è¯
        if paper.keywords and paper.keywords.keywords:
            properties["Keywords"] = {
                "multi_select": [
                    {"name": kw[:100]} for kw in paper.keywords.keywords[:5]
                ]
            }
        
        # æ ‡ç­¾
        if paper.labels and paper.labels.labels:
            properties["Labels"] = {
                "multi_select": [
                    {"name": label[:100]} for label in paper.labels.labels[:5]
                ]
            }
        
        # æŠ•ç¥¨æ•°
        if paper.hf_metadata:
            properties["Upvotes"] = {
                "number": paper.hf_metadata.metrics.upvotes
            }
            
            # ç»„ç»‡
            if paper.hf_metadata.organization:
                properties["Organization"] = {
                    "rich_text": [{"text": {"content": paper.hf_metadata.organization.name}}]
                }
            
            # æœˆä»½
            properties["Month"] = {
                "rich_text": [{"text": {"content": paper.hf_metadata.month}}]
            }
        
        # é“¾æ¥
        properties["arXiv URL"] = {
            "url": f"https://arxiv.org/abs/{paper.paper_id}"
        }
        properties["HuggingFace URL"] = {
            "url": f"https://huggingface.co/papers/{paper.paper_id}"
        }
        
        return properties
    
    def _build_page_content(self, paper: FullPaper) -> List[Dict[str, Any]]:
        """æ„å»ºé¡µé¢å†…å®¹å—"""
        blocks = []
        
        # ========== æ ‡é¢˜æ¨ªå¹… ==========
        blocks.append({
            "object": "block",
            "type": "callout",
            "callout": {
                "rich_text": [{"text": {"content": f"ğŸ“š {paper.title}"}}],
                "icon": {"emoji": "ğŸ“„"},
                "color": "blue_background"
            }
        })
        
        # ========== å…ƒä¿¡æ¯è¡¨æ ¼ ==========
        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"text": {"content": "ğŸ“‹ è®ºæ–‡ä¿¡æ¯"}}]
            }
        })
        
        # ä½œè€…
        if paper.authors:
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [
                        {"text": {"content": "ğŸ‘¥ ä½œè€…: ", "annotations": {"bold": True}}},
                        {"text": {"content": ", ".join(paper.authors[:10])}}
                    ]
                }
            })
        
        # åˆ†ç±»å’Œæ ‡ç­¾
        if paper.classification:
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [
                        {"text": {"content": "ğŸ·ï¸ åˆ†ç±»: ", "annotations": {"bold": True}}},
                        {"text": {"content": f"{paper.classification.category_name} ({paper.classification.category_name_zh})"}}
                    ]
                }
            })
        
        if paper.keywords and paper.keywords.keywords:
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [
                        {"text": {"content": "ğŸ”‘ å…³é”®è¯: ", "annotations": {"bold": True}}},
                        {"text": {"content": ", ".join(paper.keywords.keywords)}}
                    ]
                }
            })
        
        if paper.labels and paper.labels.labels:
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [
                        {"text": {"content": "ğŸ·ï¸ æ ‡ç­¾: ", "annotations": {"bold": True}}},
                        {"text": {"content": ", ".join(paper.labels.labels)}}
                    ]
                }
            })
        
        # é“¾æ¥
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": [
                    {"text": {"content": "ğŸ”— é“¾æ¥: ", "annotations": {"bold": True}}},
                    {"text": {"content": "arXiv", "link": {"url": f"https://arxiv.org/abs/{paper.paper_id}"}}},
                    {"text": {"content": " | "}},
                    {"text": {"content": "PDF", "link": {"url": f"https://arxiv.org/pdf/{paper.paper_id}.pdf"}}},
                    {"text": {"content": " | "}},
                    {"text": {"content": "ar5iv", "link": {"url": f"https://ar5iv.labs.arxiv.org/html/{paper.paper_id}"}}},
                    {"text": {"content": " | "}},
                    {"text": {"content": "HuggingFace", "link": {"url": f"https://huggingface.co/papers/{paper.paper_id}"}}},
                ]
            }
        })
        
        blocks.append({"object": "block", "type": "divider", "divider": {}})
        
        # ========== æ‘˜è¦ ==========
        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"text": {"content": "ğŸ“ æ‘˜è¦"}}]
            }
        })
        
        if paper.abstract:
            # åˆ†æ®µå¤„ç†é•¿æ‘˜è¦
            abstract_text = paper.abstract
            chunks = [abstract_text[i:i+2000] for i in range(0, len(abstract_text), 2000)]
            for chunk in chunks:
                blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"text": {"content": chunk}}]
                    }
                })
        
        blocks.append({"object": "block", "type": "divider", "divider": {}})
        
        # ========== é˜…è¯»ç¬”è®° ==========
        if paper.comments and paper.comments.comments:
            blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"text": {"content": "ğŸ“– é˜…è¯»ç¬”è®°"}}]
            }
            })
            
            # æ€»ç»“
            blocks.append({
                "object": "block",
                "type": "callout",
                "callout": {
                    "rich_text": [{"text": {"content": paper.comments.summary}}],
                    "icon": {"emoji": "ğŸ’¡"},
                    "color": "yellow_background"
                }
            })
            
            # æŒ‰ç« èŠ‚ç»„ç»‡è¯„è®º
            current_section = ""
            for comment in paper.comments.comments:
                # ç« èŠ‚æ ‡é¢˜
                if comment.section_title != current_section:
                    current_section = comment.section_title
                    blocks.append({
                        "object": "block",
                        "type": "heading_3",
                        "heading_3": {
                            "rich_text": [{"text": {"content": f"ğŸ“Œ {current_section}"}}]
                        }
                    })
                
                # é‡è¦æ€§å›¾æ ‡
                importance_emoji = {
                    "high": "ğŸ”´",
                    "medium": "ğŸŸ¡",
                    "low": "ğŸŸ¢"
                }.get(comment.importance, "âšª")
                
                # æ®µè½è¯„è®º
                blocks.append({
                    "object": "block",
                    "type": "toggle",
                    "toggle": {
                        "rich_text": [
                            {"text": {"content": f"{importance_emoji} "}},
                            {"text": {"content": truncate_text(comment.paragraph_text, 80)}}
                        ],
                        "children": [
                            # è¦ç‚¹
                            {
                                "object": "block",
                                "type": "bulleted_list_item",
                                "bulleted_list_item": {
                                    "rich_text": [
                                        {"text": {"content": "è¦ç‚¹: ", "annotations": {"bold": True}}},
                                        {"text": {"content": " | ".join(comment.key_points)}}
                                    ]
                                }
                            },
                            # é˜…è¯»ç¬”è®°
                            {
                                "object": "block",
                                "type": "bulleted_list_item",
                                "bulleted_list_item": {
                                    "rich_text": [
                                        {"text": {"content": "ç¬”è®°: ", "annotations": {"bold": True}}},
                                        {"text": {"content": comment.reading_notes}}
                                    ]
                                }
                            }
                        ]
                    }
                })
        
        # ========== è®ºæ–‡ç»“æ„ ==========
        if paper.content and paper.content.sections:
            blocks.append({"object": "block", "type": "divider", "divider": {}})
            blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"text": {"content": "ğŸ“š è®ºæ–‡ç»“æ„"}}]
                }
            })
            
            for section in paper.content.sections[:10]:  # é™åˆ¶ç« èŠ‚æ•°
                blocks.append({
                    "object": "block",
                    "type": "toggle",
                    "toggle": {
                        "rich_text": [{"text": {"content": f"ğŸ“– {section.title}"}}],
                        "children": [
                            {
                                "object": "block",
                                "type": "paragraph",
                                "paragraph": {
                                    "rich_text": [{"text": {"content": truncate_text(para, 500)}}]
                                }
                            }
                            for para in section.paragraphs[:3]
                        ] if section.paragraphs else []
                    }
                })
        
        # ========== å›¾è¡¨ ==========
        if paper.content and paper.content.figures:
            blocks.append({"object": "block", "type": "divider", "divider": {}})
            blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"text": {"content": f"ğŸ–¼ï¸ å›¾è¡¨ ({len(paper.content.figures)})"}}]
                }
            })
            
            for i, fig in enumerate(paper.content.figures[:5]):
                if fig.src.startswith("http"):
                    blocks.append({
                        "object": "block",
                        "type": "image",
                        "image": {
                            "type": "external",
                            "external": {"url": fig.src}
                        }
                    })
                    if fig.caption:
                        blocks.append({
                            "object": "block",
                            "type": "paragraph",
                            "paragraph": {
                                "rich_text": [{"text": {"content": f"Figure {i+1}: {truncate_text(fig.caption, 200)}", "annotations": {"italic": True}}}]
                            }
                        })
        
        # ========== é¡µè„š ==========
        blocks.append({"object": "block", "type": "divider", "divider": {}})
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": [
                    {"text": {"content": f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", "annotations": {"italic": True, "color": "gray"}}}
                ]
            }
        })
        
        return blocks
    
    async def create_page(self, paper: FullPaper) -> Optional[str]:
        """
        åˆ›å»ºè®ºæ–‡é¡µé¢
        
        Args:
            paper: å®Œæ•´è®ºæ–‡æ•°æ®
            
        Returns:
            é¡µé¢IDæˆ–None
        """
        try:
            properties = self._build_database_properties(paper)
            blocks = self._build_page_content(paper)
            
            # Notion APIé™åˆ¶æ¯æ¬¡æœ€å¤š100ä¸ªblocks
            blocks = blocks[:100]
            
            response = await self.client.pages.create(
                parent={"database_id": self.database_id},
                properties=properties,
                children=blocks
            )
            
            page_id = response["id"]
            self.created_count += 1
            logger.info(f"åˆ›å»ºé¡µé¢æˆåŠŸ: {paper.paper_id} -> {page_id[:8]}...")
            
            return page_id
            
        except Exception:
            error_message = format_exception()
            logger.error(f"åˆ›å»ºé¡µé¢å¤±è´¥ {paper.paper_id}: {error_message}")
            self.error_count += 1
            return None
    
    async def find_existing_page(self, paper_id: str) -> Optional[str]:
        """
        æŸ¥æ‰¾å·²å­˜åœ¨çš„é¡µé¢
        
        Args:
            paper_id: è®ºæ–‡ID
            
        Returns:
            é¡µé¢IDæˆ–None
        """
        try:
            response = await self.client.databases.query(
                database_id=self.database_id,
                filter={
                    "property": "Paper ID",
                    "rich_text": {"equals": paper_id}
                }
            )
            
            if response["results"]:
                return response["results"][0]["id"]
            return None
            
        except Exception:
            error_message = format_exception()
            logger.debug(f"æŸ¥æ‰¾é¡µé¢å¤±è´¥ {paper_id}: {error_message}")
            return None
    
    async def update_page(self, page_id: str, paper: FullPaper) -> bool:
        """
        æ›´æ–°é¡µé¢å±æ€§
        
        Args:
            page_id: é¡µé¢ID
            paper: è®ºæ–‡æ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            properties = self._build_database_properties(paper)
            
            await self.client.pages.update(
                page_id=page_id,
                properties=properties
            )
            
            self.updated_count += 1
            logger.info(f"æ›´æ–°é¡µé¢æˆåŠŸ: {paper.paper_id}")
            return True
            
        except Exception:
            error_message = format_exception()
            logger.error(f"æ›´æ–°é¡µé¢å¤±è´¥ {paper.paper_id}: {error_message}")
            self.error_count += 1
            return False
    
    async def sync_paper(
        self,
        paper: FullPaper,
        update_existing: bool = False
    ) -> Optional[str]:
        """
        åŒæ­¥è®ºæ–‡åˆ°Notion
        
        Args:
            paper: è®ºæ–‡æ•°æ®
            update_existing: æ˜¯å¦æ›´æ–°å·²å­˜åœ¨çš„é¡µé¢
            
        Returns:
            é¡µé¢IDæˆ–None
        """
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
        existing_id = await self.find_existing_page(paper.paper_id)
        
        if existing_id:
            if update_existing:
                success = await self.update_page(existing_id, paper)
                return existing_id if success else None
            else:
                logger.debug(f"é¡µé¢å·²å­˜åœ¨ï¼Œè·³è¿‡: {paper.paper_id}")
                return existing_id
        else:
            return await self.create_page(paper)
    
    async def sync_papers(
        self,
        papers: List[FullPaper],
        update_existing: bool = False,
        delay: float = 0.5
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡åŒæ­¥è®ºæ–‡
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            update_existing: æ˜¯å¦æ›´æ–°å·²å­˜åœ¨çš„
            delay: è¯·æ±‚é—´éš”
            
        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info(f"å¼€å§‹åŒæ­¥ {len(papers)} ç¯‡è®ºæ–‡åˆ°Notion")
        
        results = {"synced": [], "failed": [], "skipped": []}
        
        for i, paper in enumerate(papers):
            logger.info(f"åŒæ­¥è¿›åº¦: {i+1}/{len(papers)}")
            
            try:
                page_id = await self.sync_paper(paper, update_existing)
                
                if page_id:
                    results["synced"].append(paper.paper_id)
                    paper.notion_page_id = page_id
                    paper.notion_synced_at = datetime.now()
                else:
                    results["failed"].append(paper.paper_id)
                    
            except Exception:
                error_message = format_exception()
                logger.error(f"åŒæ­¥å¤±è´¥ {paper.paper_id}: {error_message}")
                results["failed"].append(paper.paper_id)
            
            # é¿å…é€Ÿç‡é™åˆ¶
            await asyncio.sleep(delay)
        
        logger.info(
            f"åŒæ­¥å®Œæˆ: æˆåŠŸ {len(results['synced'])}, "
            f"å¤±è´¥ {len(results['failed'])}, "
            f"è·³è¿‡ {len(results['skipped'])}"
        )
        
        return results
    
    def get_stats(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡"""
        return {
            "created": self.created_count,
            "updated": self.updated_count,
            "errors": self.error_count
        }


async def setup_database_schema(client: NotionPaperClient) -> bool:
    """
    è®¾ç½®æ•°æ®åº“schemaï¼ˆéœ€è¦æ‰‹åŠ¨åœ¨Notionä¸­åˆ›å»ºï¼‰
    
    æ•°æ®åº“åº”åŒ…å«ä»¥ä¸‹å±æ€§:
    - Title (title): è®ºæ–‡æ ‡é¢˜
    - Paper ID (rich_text): arXiv ID
    - Authors (rich_text): ä½œè€…
    - Category (select): åˆ†ç±»
    - Keywords (multi_select): å…³é”®è¯
    - Labels (multi_select): æ ‡ç­¾
    - Upvotes (number): ç‚¹èµæ•°
    - Organization (rich_text): ç»„ç»‡
    - Month (rich_text): æœˆä»½
    - arXiv URL (url): arXivé“¾æ¥
    - HuggingFace URL (url): HFé“¾æ¥
    """
    logger.info("""
è¯·ç¡®ä¿Notionæ•°æ®åº“åŒ…å«ä»¥ä¸‹å±æ€§:
- Title (title): è®ºæ–‡æ ‡é¢˜
- Paper ID (rich_text): arXiv ID  
- Authors (rich_text): ä½œè€…
- Category (select): åˆ†ç±»
- Keywords (multi_select): å…³é”®è¯
- Labels (multi_select): æ ‡ç­¾
- Upvotes (number): ç‚¹èµæ•°
- Organization (rich_text): ç»„ç»‡
- Month (rich_text): æœˆä»½
- arXiv URL (url): arXivé“¾æ¥
- HuggingFace URL (url): HFé“¾æ¥
    """)
    return True


async def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºç‹¬ç«‹æµ‹è¯•"""
    from utils import setup_logging
    
    setup_logging()
    settings.ensure_directories()
    
    logger.info("å¼€å§‹Notionå®¢æˆ·ç«¯æµ‹è¯•")
    
    # æ£€æŸ¥é…ç½®
    if not settings.notion_token:
        logger.error("è¯·è®¾ç½® NOTION_TOKEN ç¯å¢ƒå˜é‡")
        return
    if not settings.notion_database_id:
        logger.error("è¯·è®¾ç½® NOTION_DATABASE_ID ç¯å¢ƒå˜é‡")
        return
    
    client = NotionPaperClient()
    
    # æµ‹è¯•è¿æ¥
    if not await client.check_connection():
        return
    
    # åˆ›å»ºæµ‹è¯•è®ºæ–‡
    from models import FullPaper, HFPaper, PaperMetrics, ClassificationResult, KeywordsResult
    
    test_paper = FullPaper(
        paper_id="1706.03762",
        title="Attention Is All You Need",
        authors=["Ashish Vaswani", "Noam Shazeer", "Niki Parmar"],
        abstract="The dominant sequence transduction models are based on complex recurrent or convolutional neural networks...",
        hf_metadata=HFPaper(
            paper_id="1706.03762",
            title="Attention Is All You Need",
            url="https://huggingface.co/papers/1706.03762",
            arxiv_url="https://arxiv.org/abs/1706.03762",
            ar5iv_url="https://ar5iv.labs.arxiv.org/html/1706.03762",
            month="2017-06",
            metrics=PaperMetrics(upvotes=1000, comments=50)
        ),
        classification=ClassificationResult(
            paper_id="1706.03762",
            category="language_models",
            category_name="Language Models",
            category_name_zh="è¯­è¨€æ¨¡å‹",
            confidence=0.95,
            raw_response=""
        ),
        keywords=KeywordsResult(
            paper_id="1706.03762",
            keywords=["transformer", "attention", "encoder-decoder", "machine translation"],
            raw_response=""
        )
    )
    
    # åŒæ­¥æµ‹è¯•
    page_id = await client.sync_paper(test_paper)
    if page_id:
        logger.info(f"æµ‹è¯•é¡µé¢åˆ›å»ºæˆåŠŸ: {page_id}")
    
    logger.info(f"ç»Ÿè®¡: {client.get_stats()}")


if __name__ == "__main__":
    asyncio.run(main())