"""
scraper_hf.py - HuggingFace Papers çˆ¬è™«æ¨¡å— (å¸¦ä»£ç†æ”¯æŒ)

æ•´åˆä»£ç†ç®¡ç†å™¨ï¼Œæ”¯æŒ:
- è‡ªåŠ¨ä»£ç†åˆ‡æ¢å’Œæ•…éšœè½¬ç§»
- æ™ºèƒ½é‡è¯•æœºåˆ¶
- è¯·æ±‚é¢‘ç‡æ§åˆ¶
- å®Œå–„çš„é”™è¯¯å¤„ç†

HTMLç»“æ„å‚è€ƒ (2024-12):
- article.relative: è®ºæ–‡å¡ç‰‡å®¹å™¨
- a[href^="/papers/"]: è®ºæ–‡é“¾æ¥ï¼ŒåŒ…å«paper_id
- h3 > a.line-clamp-3: è®ºæ–‡æ ‡é¢˜
- label > div.leading-none: æŠ•ç¥¨æ•°
"""

import re
import sys
import asyncio
import traceback
from datetime import datetime
from typing import List, Optional, Dict, Any, Callable
from functools import wraps

import aiohttp
from bs4 import BeautifulSoup
from loguru import logger

# æœ¬åœ°æ¨¡å—
from proxy_manager import ProxyManager, create_proxy_manager

# å°è¯•å¯¼å…¥é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from config import settings
except ImportError:
    settings = None

# å°è¯•å¯¼å…¥æ¨¡å‹å®šä¹‰ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from models import HFPaper, PaperMetrics, Organization, ScrapingStats
except ImportError:
    # å†…è”å®šä¹‰åŸºæœ¬æ¨¡å‹
    from dataclasses import dataclass, field
    from typing import Optional
    
    @dataclass
    class PaperMetrics:
        upvotes: int = 0
        comments: int = 0
        github_stars: Optional[int] = None
    
    @dataclass
    class Organization:
        name: str
        logo: Optional[str] = None
        url: Optional[str] = None
    
    @dataclass
    class HFPaper:
        paper_id: str
        title: str
        url: str
        arxiv_url: str
        ar5iv_url: str
        month: str
        thumbnail: Optional[str] = None
        submitter: Optional[str] = None
        organization: Optional[Organization] = None
        metrics: PaperMetrics = field(default_factory=PaperMetrics)
        has_video: bool = False
        
        def model_dump(self) -> dict:
            """è½¬æ¢ä¸ºå­—å…¸"""
            return {
                'paper_id': self.paper_id,
                'title': self.title,
                'url': self.url,
                'arxiv_url': self.arxiv_url,
                'ar5iv_url': self.ar5iv_url,
                'month': self.month,
                'thumbnail': self.thumbnail,
                'submitter': self.submitter,
                'organization': {
                    'name': self.organization.name,
                    'logo': self.organization.logo,
                    'url': self.organization.url,
                } if self.organization else None,
                'metrics': {
                    'upvotes': self.metrics.upvotes,
                    'comments': self.metrics.comments,
                    'github_stars': self.metrics.github_stars,
                },
                'has_video': self.has_video,
            }
    
    @dataclass
    class ScrapingStats:
        month: str
        start_time: datetime = field(default_factory=datetime.now)
        end_time: Optional[datetime] = None
        total_papers: int = 0
        filtered_papers: int = 0
        
        @property
        def duration_seconds(self) -> float:
            if self.end_time:
                return (self.end_time - self.start_time).total_seconds()
            return 0


# ============================================
# å·¥å…·å‡½æ•°
# ============================================

def build_hf_monthly_url(month: str) -> str:
    """æ„å»ºHuggingFaceæœˆåº¦è®ºæ–‡URL"""
    return f"https://huggingface.co/papers?date={month}"


def build_arxiv_url(paper_id: str) -> str:
    """æ„å»ºarXiv URL"""
    return f"https://arxiv.org/abs/{paper_id}"


def build_ar5iv_url(paper_id: str) -> str:
    """æ„å»ºar5iv URL (HTMLç‰ˆarXiv)"""
    return f"https://ar5iv.org/abs/{paper_id}"


def generate_months(start: str, end: str):
    """ç”Ÿæˆæœˆä»½èŒƒå›´"""
    from datetime import datetime
    
    start_date = datetime.strptime(start, "%Y-%m")
    end_date = datetime.strptime(end, "%Y-%m")
    
    current = start_date
    while current <= end_date:
        yield current.strftime("%Y-%m")
        # ç§»åŠ¨åˆ°ä¸‹ä¸ªæœˆ
        if current.month == 12:
            current = current.replace(year=current.year + 1, month=1)
        else:
            current = current.replace(month=current.month + 1)


def format_exception() -> str:
    """æ ¼å¼åŒ–å¼‚å¸¸ä¿¡æ¯"""
    return traceback.format_exc().strip().split('\n')[-1]


async def save_jsonl(data: List[dict], filepath: str):
    """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
    import json
    from pathlib import Path
    
    Path(filepath).parent.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')


async def load_jsonl(filepath: str) -> List[dict]:
    """åŠ è½½JSONLæ–‡ä»¶"""
    import json
    from pathlib import Path
    
    if not Path(filepath).exists():
        return []
    
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data


# ============================================
# é‡è¯•è£…é¥°å™¨
# ============================================

def async_retry(
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    exceptions: tuple = (aiohttp.ClientError, asyncio.TimeoutError),
    on_retry: Optional[Callable] = None,
):
    """
    å¼‚æ­¥é‡è¯•è£…é¥°å™¨
    
    Args:
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        base_delay: åŸºç¡€å»¶è¿Ÿï¼ˆç§’ï¼‰
        max_delay: æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
        exceptions: éœ€è¦é‡è¯•çš„å¼‚å¸¸ç±»å‹
        on_retry: é‡è¯•æ—¶çš„å›è°ƒå‡½æ•°
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return await func(*args, **kwargs)
                    
                except exceptions as e:
                    last_exception = e
                    
                    if attempt < max_retries:
                        # æŒ‡æ•°é€€é¿
                        delay = min(base_delay * (2 ** attempt), max_delay)
                        
                        logger.warning(
                            f"é‡è¯• {attempt + 1}/{max_retries}: {e.__class__.__name__} - "
                            f"ç­‰å¾… {delay:.1f}s"
                        )
                        
                        if on_retry:
                            on_retry(attempt, e)
                        
                        await asyncio.sleep(delay)
                    else:
                        logger.error(f"é‡è¯•æ¬¡æ•°å·²ç”¨å°½: {e}")
            
            raise last_exception
        
        return wrapper
    return decorator


# ============================================
# HuggingFace çˆ¬è™«
# ============================================

class HFPapersScraper:
    """
    HuggingFace Papers çˆ¬è™« (å¸¦ä»£ç†æ”¯æŒ)
    
    Features:
    - è‡ªåŠ¨ä»£ç†ç®¡ç†å’Œæ•…éšœè½¬ç§»
    - æ™ºèƒ½é‡è¯•æœºåˆ¶
    - å¼‚æ­¥å¹¶å‘æ§åˆ¶
    - å®Œå–„çš„HTMLè§£æ
    """
    
    DEFAULT_USER_AGENT = (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
    
    def __init__(
        self,
        proxy_manager: Optional[ProxyManager] = None,
        min_votes: int = 50,
        concurrency: int = 3,
        request_delay: float = 1.0,
        max_retries: int = 3,
        user_agent: str = None,
    ):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            proxy_manager: ä»£ç†ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
            min_votes: æœ€å°æŠ•ç¥¨æ•°é˜ˆå€¼
            concurrency: å¹¶å‘æ•°
            request_delay: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            user_agent: ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
        """
        self.proxy_manager = proxy_manager
        self.min_votes = min_votes
        self.concurrency = concurrency
        self.request_delay = request_delay
        self.max_retries = max_retries
        self.user_agent = user_agent or self.DEFAULT_USER_AGENT
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats: Dict[str, ScrapingStats] = {}
        
        # ä¼šè¯ï¼ˆå»¶è¿Ÿåˆ›å»ºï¼‰
        self._session: Optional[aiohttp.ClientSession] = None
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """è·å–æˆ–åˆ›å»ºaiohttpä¼šè¯"""
        if self._session is None or self._session.closed:
            if self.proxy_manager:
                self._session = self.proxy_manager.create_session(timeout=30)
            else:
                self._session = aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=30)
                )
        return self._session
    
    async def close(self):
        """å…³é—­ä¼šè¯"""
        if self._session and not self._session.closed:
            await self._session.close()
            self._session = None
    
    def _get_headers(self) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´"""
        return {
            "User-Agent": self.user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
    
    async def _fetch_page(
        self,
        url: str,
        retry_count: int = 0
    ) -> Optional[str]:
        """
        è·å–é¡µé¢HTMLï¼ˆå¸¦é‡è¯•ï¼‰
        
        Args:
            url: é¡µé¢URL
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°
            
        Returns:
            HTMLå†…å®¹æˆ–None
        """
        session = await self._get_session()
        headers = self._get_headers()
        
        # è·å–ä»£ç†å‚æ•°
        request_kwargs = {}
        if self.proxy_manager:
            request_kwargs = self.proxy_manager.get_request_kwargs()
        
        try:
            async with session.get(
                url, 
                headers=headers, 
                **request_kwargs
            ) as response:
                
                if response.status == 200:
                    # æ­£ç¡®å¤„ç†ç¼–ç 
                    content = await response.read()
                    try:
                        html = content.decode('utf-8')
                    except UnicodeDecodeError:
                        html = content.decode('utf-8', errors='replace')
                    
                    # æŠ¥å‘ŠæˆåŠŸ
                    if self.proxy_manager:
                        self.proxy_manager.report_success()
                    
                    return html
                
                elif response.status == 429:
                    # é€Ÿç‡é™åˆ¶
                    wait_time = min(60 * (retry_count + 1), 300)
                    logger.warning(f"âš ï¸ é€Ÿç‡é™åˆ¶ (429)ï¼Œç­‰å¾… {wait_time}s: {url}")
                    
                    # å°è¯•åˆ‡æ¢ä»£ç†
                    if self.proxy_manager:
                        self.proxy_manager.report_failure()
                    
                    await asyncio.sleep(wait_time)
                    
                    if retry_count < self.max_retries:
                        return await self._fetch_page(url, retry_count + 1)
                
                elif response.status == 403:
                    logger.error(f"âŒ è®¿é—®è¢«æ‹’ç» (403): {url}")
                    if self.proxy_manager:
                        self.proxy_manager.report_failure()
                
                else:
                    logger.warning(f"âš ï¸ HTTP {response.status}: {url}")
                    
        except asyncio.TimeoutError:
            logger.warning(f"â±ï¸ è¯·æ±‚è¶…æ—¶: {url}")
            if self.proxy_manager:
                self.proxy_manager.report_failure()
            
            if retry_count < self.max_retries:
                await asyncio.sleep(5 * (retry_count + 1))
                return await self._fetch_page(url, retry_count + 1)
                
        except aiohttp.ClientError as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            if self.proxy_manager:
                self.proxy_manager.report_failure()
            
            if retry_count < self.max_retries:
                await asyncio.sleep(5 * (retry_count + 1))
                return await self._fetch_page(url, retry_count + 1)
        
        except Exception as e:
            logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {format_exception()}")
            
        return None
    
    def _parse_papers(self, html: str, month: str) -> List[HFPaper]:
        """
        è§£æHTMLæå–è®ºæ–‡åˆ—è¡¨
        
        åŸºäºå®é™…HTMLç»“æ„ï¼š
        - article.relative: è®ºæ–‡å¡ç‰‡å®¹å™¨
        - h3 > a[href^="/papers/"]: æ ‡é¢˜é“¾æ¥
        - label > div.leading-none: æŠ•ç¥¨æ•°
        """
        soup = BeautifulSoup(html, "html.parser")
        papers = []
        seen_ids = set()
        
        # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰articleå®¹å™¨
        articles = soup.find_all("article", class_=re.compile(r"relative"))
        
        if articles:
            for article in articles:
                paper = self._parse_article_card(article, month, seen_ids)
                if paper:
                    papers.append(paper)
        
        # æ–¹æ³•2: å›é€€åˆ°æŸ¥æ‰¾h3å†…çš„æ ‡é¢˜é“¾æ¥
        if not papers:
            h3_tags = soup.find_all("h3")
            for h3 in h3_tags:
                link = h3.find("a", href=re.compile(r"^/papers/\d{4}\.\d{4,5}"))
                if not link:
                    continue
                paper = self._parse_from_title_link(link, month, seen_ids)
                if paper:
                    papers.append(paper)
        
        return papers
    
    def _parse_article_card(
        self, 
        article, 
        month: str, 
        seen_ids: set
    ) -> Optional[HFPaper]:
        """ä»articleå¡ç‰‡è§£æè®ºæ–‡ä¿¡æ¯"""
        try:
            h3 = article.find("h3")
            if not h3:
                return None
            
            title_link = h3.find("a", href=re.compile(r"^/papers/\d{4}\.\d{4,5}"))
            if not title_link:
                return None
            
            paper_id = title_link["href"].split("/")[-1]
            
            if paper_id in seen_ids:
                return None
            seen_ids.add(paper_id)
            
            title = title_link.get_text(strip=True)
            if not title or len(title) < 5:
                return None
            
            # æå–å„é¡¹ä¿¡æ¯
            thumbnail = self._extract_thumbnail(article)
            submitter = self._extract_submitter(article)
            upvotes = self._extract_upvotes(article, paper_id)
            organization = self._extract_organization(article, title_link)
            comments = self._extract_comments(article, paper_id)
            github_stars = self._extract_github_stars(article)
            has_video = self._check_has_video(article)
            
            return HFPaper(
                paper_id=paper_id,
                title=title,
                url=f"https://huggingface.co/papers/{paper_id}",
                arxiv_url=build_arxiv_url(paper_id),
                ar5iv_url=build_ar5iv_url(paper_id),
                thumbnail=thumbnail,
                submitter=submitter,
                organization=organization,
                metrics=PaperMetrics(
                    upvotes=upvotes, 
                    comments=comments,
                    github_stars=github_stars
                ),
                has_video=has_video,
                month=month,
            )
            
        except Exception:
            logger.debug(f"è§£æarticleå¡ç‰‡å¤±è´¥: {format_exception()}")
            return None
    
    def _parse_from_title_link(
        self,
        link,
        month: str,
        seen_ids: set
    ) -> Optional[HFPaper]:
        """ä»æ ‡é¢˜é“¾æ¥è§£æè®ºæ–‡ï¼ˆå›é€€æ–¹æ³•ï¼‰"""
        try:
            paper_id = link["href"].split("/")[-1]
            
            if paper_id in seen_ids:
                return None
            seen_ids.add(paper_id)
            
            title = link.get_text(strip=True)
            if not title or len(title) < 5:
                return None
            
            container = self._find_paper_container(link)
            if not container:
                return None
            
            return HFPaper(
                paper_id=paper_id,
                title=title,
                url=f"https://huggingface.co/papers/{paper_id}",
                arxiv_url=build_arxiv_url(paper_id),
                ar5iv_url=build_ar5iv_url(paper_id),
                thumbnail=self._extract_thumbnail(container),
                submitter=self._extract_submitter(container),
                organization=self._extract_organization(container, link),
                metrics=PaperMetrics(
                    upvotes=self._extract_upvotes(container, paper_id), 
                    comments=self._extract_comments(container, paper_id),
                    github_stars=self._extract_github_stars(container)
                ),
                has_video=self._check_has_video(container),
                month=month,
            )
            
        except Exception:
            logger.debug(f"è§£ææ ‡é¢˜é“¾æ¥å¤±è´¥: {format_exception()}")
            return None
    
    def _find_paper_container(self, link) -> Optional[Any]:
        """æŸ¥æ‰¾è®ºæ–‡å¡ç‰‡å®¹å™¨"""
        container = link.parent
        max_depth = 10
        depth = 0
        
        while container and container.name != "body" and depth < max_depth:
            if container.find("img", src=re.compile(r"cdn-thumbnails|cdn-uploads")):
                return container
            container = container.parent
            depth += 1
        
        return link.parent
    
    def _extract_thumbnail(self, container) -> Optional[str]:
        """æå–ç¼©ç•¥å›¾URL"""
        img = container.find("img", src=re.compile(r"cdn-thumbnails"))
        return img["src"] if img else None
    
    def _extract_submitter(self, container) -> Optional[str]:
        """æå–æäº¤è€…"""
        # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«"Submitted by"çš„div
        divs = container.find_all("div", string=re.compile(r"Submitted by", re.I))
        for div in divs:
            full_text = div.get_text(separator=" ", strip=True)
            match = re.search(r"Submitted by\s+(.+)", full_text, re.I)
            if match:
                return match.group(1).strip()
        
        # æ–¹æ³•2: æŸ¥æ‰¾æ–‡æœ¬èŠ‚ç‚¹
        submitter_text = container.find(string=re.compile(r"Submitted by", re.I))
        if submitter_text:
            parent = submitter_text.parent
            if parent:
                texts = []
                for child in parent.children:
                    if isinstance(child, str):
                        text = child.strip()
                        if text and "Submitted by" not in text:
                            texts.append(text)
                
                if texts:
                    return texts[-1]
                
                full_text = parent.get_text(separator=" ", strip=True)
                match = re.search(r"Submitted by\s+(.+)", full_text, re.I)
                if match:
                    return match.group(1).strip()
        
        return None
    
    def _extract_upvotes(self, container, paper_id: str) -> int:
        """æå–ç‚¹èµæ•°"""
        # æ–¹æ³•1: æŸ¥æ‰¾labelå†…çš„div.leading-none
        label = container.find("label", class_=re.compile(r"rounded-xl|cursor-pointer"))
        if label:
            vote_div = label.find("div", class_=re.compile(r"leading-none"))
            if vote_div:
                text = vote_div.get_text(strip=True)
                if text.isdigit():
                    return int(text)
                match = re.match(r"([\d.]+)k?", text, re.I)
                if match:
                    num = float(match.group(1))
                    if "k" in text.lower():
                        num *= 1000
                    return int(num)
        
        # æ–¹æ³•2: æŸ¥æ‰¾åŒ…å«æŠ•ç¥¨å›¾æ ‡çš„å®¹å™¨
        vote_containers = container.find_all("label")
        for vc in vote_containers:
            svg = vc.find("svg")
            if svg:
                path = svg.find("path", d=re.compile(r"M5\.19|triangle", re.I))
                if path or (svg.get("viewBox") == "0 0 12 12"):
                    text = vc.get_text(strip=True)
                    numbers = re.findall(r"\d+", text)
                    if numbers:
                        return int(numbers[-1])
        
        # æ–¹æ³•3: å›é€€åˆ°æŸ¥æ‰¾ç™»å½•é“¾æ¥
        login_link = container.find("a", href=re.compile(rf"/login.*next.*{re.escape(paper_id)}"))
        if login_link:
            text = login_link.get_text(strip=True)
            if text.isdigit():
                return int(text)
        
        return 0
    
    def _extract_organization(self, container, paper_link) -> Optional[Organization]:
        """æå–ç»„ç»‡ä¿¡æ¯"""
        org_links = container.find_all("a", class_=re.compile(r"bg-blue|border-blue"))
        for org_link in org_links:
            href = org_link.get("href", "")
            if "/papers/" in href or "#" in href or href.startswith("http"):
                continue
            if not re.match(r"^/[\w-]+$", href):
                continue
            
            span = org_link.find("span")
            if span:
                name = span.get_text(strip=True)
            else:
                name = org_link.get_text(strip=True)
            
            if name and len(name) > 1:
                org_img = org_link.find("img")
                return Organization(
                    name=name,
                    logo=org_img.get("src") if org_img else None,
                    url=f"https://huggingface.co{href}"
                )
        
        return None
    
    def _extract_comments(self, container, paper_id: str) -> int:
        """æå–è¯„è®ºæ•°"""
        comment_link = container.find("a", href=re.compile(rf"/papers/{re.escape(paper_id)}#community"))
        if comment_link:
            text = comment_link.get_text(strip=True)
            match = re.search(r"(\d+)", text)
            if match:
                return int(match.group(1))
        return 0
    
    def _extract_github_stars(self, container) -> Optional[int]:
        """æå–GitHubæ˜Ÿæ ‡æ•°"""
        links = container.find_all("a", class_=re.compile(r"items-center"))
        for link in links:
            svg = link.find("svg")
            if not svg:
                continue
            
            viewbox = svg.get("viewBox", "")
            if "256 250" not in viewbox:
                path = svg.find("path", d=re.compile(r"M128\.001|github", re.I))
                if not path:
                    continue
            
            text = link.get_text(strip=True)
            match = re.match(r"([\d.]+)\s*k?", text, re.I)
            if match:
                num = float(match.group(1))
                if "k" in text.lower():
                    num *= 1000
                return int(num)
        
        return None
    
    def _check_has_video(self, container) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘"""
        if container.find("video"):
            return True
        if container.find("a", href=re.compile(r"\.(mp4|qt|webm)$", re.I)):
            return True
        return False
    
    async def scrape_month(self, month: str) -> List[HFPaper]:
        """
        çˆ¬å–å•ä¸ªæœˆä»½çš„è®ºæ–‡
        
        Args:
            month: æœˆä»½ YYYY-MM
            
        Returns:
            è¿‡æ»¤åçš„è®ºæ–‡åˆ—è¡¨
        """
        url = build_hf_monthly_url(month)
        logger.info(f"ğŸ“„ å¼€å§‹çˆ¬å– {month}: {url}")
        
        stats = ScrapingStats(month=month)
        
        html = await self._fetch_page(url)
        if not html:
            logger.error(f"âŒ æ— æ³•è·å–é¡µé¢: {url}")
            stats.end_time = datetime.now()
            self.stats[month] = stats
            return []
        
        # è§£æè®ºæ–‡
        papers = self._parse_papers(html, month)
        stats.total_papers = len(papers)
        
        # è¿‡æ»¤ä½æŠ•ç¥¨è®ºæ–‡
        filtered = [p for p in papers if p.metrics.upvotes >= self.min_votes]
        stats.filtered_papers = len(filtered)
        stats.end_time = datetime.now()
        
        self.stats[month] = stats
        
        logger.info(
            f"âœ… å®Œæˆ {month}: å‘ç° {stats.total_papers} ç¯‡, "
            f"è¿‡æ»¤å {stats.filtered_papers} ç¯‡ (>= {self.min_votes} votes)"
        )
        
        # æ·»åŠ å»¶è¿Ÿ
        await asyncio.sleep(self.request_delay)
        
        return filtered
    
    async def scrape_range(
        self,
        start_month: str,
        end_month: str,
        save_dir: str = "./data/hf_papers"
    ) -> List[HFPaper]:
        """
        çˆ¬å–æœˆä»½èŒƒå›´å†…çš„è®ºæ–‡
        
        Args:
            start_month: èµ·å§‹æœˆä»½ YYYY-MM
            end_month: ç»“æŸæœˆä»½ YYYY-MM
            save_dir: ä¿å­˜ç›®å½•
            
        Returns:
            æ‰€æœ‰è®ºæ–‡åˆ—è¡¨
        """
        months = list(generate_months(start_month, end_month))
        logger.info(f"ğŸ“… å‡†å¤‡çˆ¬å– {len(months)} ä¸ªæœˆä»½: {months[0]} åˆ° {months[-1]}")
        
        all_papers = []
        semaphore = asyncio.Semaphore(self.concurrency)
        
        async def bounded_scrape(month: str) -> List[HFPaper]:
            async with semaphore:
                papers = await self.scrape_month(month)
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                if papers and save_dir:
                    from pathlib import Path
                    filepath = Path(save_dir) / f"{month}.jsonl"
                    await save_jsonl([p.model_dump() for p in papers], str(filepath))
                
                return papers
        
        try:
            tasks = [bounded_scrape(month) for month in months]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"âŒ çˆ¬å– {months[i]} å¤±è´¥: {result}")
                else:
                    all_papers.extend(result)
        
        finally:
            await self.close()
        
        logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆ: å…± {len(all_papers)} ç¯‡è®ºæ–‡")
        
        return all_papers
    
    def get_stats_summary(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        total_papers = sum(s.total_papers for s in self.stats.values())
        filtered_papers = sum(s.filtered_papers for s in self.stats.values())
        total_duration = sum(s.duration_seconds for s in self.stats.values())
        
        return {
            "months_scraped": len(self.stats),
            "total_papers_found": total_papers,
            "papers_after_filter": filtered_papers,
            "filter_threshold": self.min_votes,
            "total_duration_seconds": total_duration,
            "proxy_enabled": self.proxy_manager is not None,
            "per_month_stats": {
                month: {
                    "total": s.total_papers,
                    "filtered": s.filtered_papers,
                    "duration": s.duration_seconds
                }
                for month, s in self.stats.items()
            }
        }


# ============================================
# ä¾¿æ·å‡½æ•°
# ============================================

async def create_scraper_with_proxy(
    proxy_config: str = None,
    min_votes: int = 50,
    test_proxy: bool = True,
) -> HFPapersScraper:
    """
    åˆ›å»ºå¸¦ä»£ç†çš„çˆ¬è™«å®ä¾‹
    
    Args:
        proxy_config: ä»£ç†é…ç½®æ–‡ä»¶è·¯å¾„æˆ–è®¢é˜…URL
        min_votes: æœ€å°æŠ•ç¥¨æ•°
        test_proxy: æ˜¯å¦æµ‹è¯•ä»£ç†
    """
    proxy_manager = None
    
    if proxy_config:
        logger.info(f"ğŸ”§ åˆå§‹åŒ–ä»£ç†: {proxy_config[:50]}...")
        proxy_manager = await create_proxy_manager(
            proxy_config,
            test_nodes=test_proxy,
            check_local=True,
        )
    
    return HFPapersScraper(
        proxy_manager=proxy_manager,
        min_votes=min_votes,
    )


# ============================================
# ä¸»å‡½æ•°
# ============================================

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    import os
    
    parser = argparse.ArgumentParser(description="HuggingFace Papers Scraper")
    parser.add_argument("--start", default="2025-01", help="èµ·å§‹æœˆä»½ (YYYY-MM)")
    parser.add_argument("--end", default="2025-01", help="ç»“æŸæœˆä»½ (YYYY-MM)")
    parser.add_argument("--min-votes", type=int, default=50, help="æœ€å°æŠ•ç¥¨æ•°")
    parser.add_argument("--proxy", help="ä»£ç†é…ç½®æ–‡ä»¶æˆ–è®¢é˜…URL")
    parser.add_argument("--output", default="./data/hf_papers", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--concurrency", type=int, default=3, help="å¹¶å‘æ•°")
    parser.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼")
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logger.remove()
    log_level = "DEBUG" if args.debug else "INFO"
    logger.add(
        sys.stderr,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}",
        level=log_level
    )
    
    print("=" * 60)
    print("ğŸš€ HuggingFace Papers Scraper")
    print("=" * 60)
    
    # åˆ›å»ºçˆ¬è™«
    proxy_manager = None
    
    if args.proxy:
        from proxy_manager import create_proxy_manager
        proxy_manager = await create_proxy_manager(
            args.proxy,
            test_nodes=True,
            check_local=True,
        )
        
        # æ˜¾ç¤ºä»£ç†çŠ¶æ€
        status = proxy_manager.get_status()
        print(f"\nğŸ“Š ä»£ç†çŠ¶æ€:")
        print(f"  å¯ç”¨èŠ‚ç‚¹: {status['available_nodes']}/{status['total_nodes']}")
        print(f"  å½“å‰ä»£ç†: {status['proxy_url']}")
    
    scraper = HFPapersScraper(
        proxy_manager=proxy_manager,
        min_votes=args.min_votes,
        concurrency=args.concurrency,
    )
    
    # å¼€å§‹çˆ¬å–
    print(f"\nğŸ“… çˆ¬å–èŒƒå›´: {args.start} ~ {args.end}")
    print(f"ğŸ“Š æŠ•ç¥¨é˜ˆå€¼: >= {args.min_votes}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output}")
    print()
    
    papers = await scraper.scrape_range(
        args.start,
        args.end,
        save_dir=args.output
    )
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = scraper.get_stats_summary()
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  çˆ¬å–æœˆä»½: {stats['months_scraped']}")
    print(f"  å‘ç°è®ºæ–‡: {stats['total_papers_found']}")
    print(f"  è¿‡æ»¤å: {stats['papers_after_filter']}")
    print(f"  æ€»è€—æ—¶: {stats['total_duration_seconds']:.1f}s")
    
    # æ˜¾ç¤ºTop 10
    if papers:
        print(f"\nğŸ† Top 10 è®ºæ–‡:")
        papers.sort(key=lambda p: p.metrics.upvotes, reverse=True)
        for i, paper in enumerate(papers[:10], 1):
            print(f"  {i}. [{paper.metrics.upvotes:4d}] {paper.title[:60]}...")


if __name__ == "__main__":
    asyncio.run(main())