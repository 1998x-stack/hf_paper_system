{
  "paper_id": "2511.21689",
  "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
  "authors": [
    "Hongjin Su‚àó1,2 Shizhe Diao‚àó1 Ximing Lu1 Mingjie Liu1 Jiacheng Xu1 Xin Dong1 Yonggan Fu1 Peter Belcak1 Hanrong Ye1 Hongxu Yin1 Yi Dong1 Evelina Bakhturina1 Tao Yu2 Yejin Choi1 Jan Kautz1 Pavlo Molchanov1 1NVIDIA",
    "2University of Hong Kong"
  ],
  "abstract": "Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity‚Äôs Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On œÑ2\\tau^{2}-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems. Code Model Data Webpage",
  "hf_metadata": {
    "paper_id": "2511.21689",
    "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
    "url": "https://huggingface.co/papers/2511.21689",
    "arxiv_url": "https://arxiv.org/abs/2511.21689",
    "ar5iv_url": "https://ar5iv.labs.arxiv.org/html/2511.21689",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21689.png",
    "submitter": "shizhediao",
    "organization": {
      "name": "NVIDIA",
      "logo": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png",
      "url": "https://huggingface.co/nvidia"
    },
    "metrics": {
      "upvotes": 109,
      "comments": 4,
      "downloads": null,
      "github_stars": null
    },
    "has_video": false,
    "month": "2025-12",
    "scraped_at": "2025-12-26 08:33:08.444238"
  },
  "content": {
    "paper_id": "2511.21689",
    "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
    "authors": [
      "Hongjin Su‚àó1,2 Shizhe Diao‚àó1 Ximing Lu1 Mingjie Liu1 Jiacheng Xu1 Xin Dong1 Yonggan Fu1 Peter Belcak1 Hanrong Ye1 Hongxu Yin1 Yi Dong1 Evelina Bakhturina1 Tao Yu2 Yejin Choi1 Jan Kautz1 Pavlo Molchanov1 1NVIDIA",
      "2University of Hong Kong"
    ],
    "affiliations": [],
    "abstract": "Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity‚Äôs Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On œÑ2\\tau^{2}-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems. Code Model Data Webpage",
    "sections": [
      {
        "title": "1 Introduction",
        "level": 2,
        "paragraphs": [
          "Large language models (LLMs) have been reported to have made remarkable strides towards superhuman intelligence but remain of limited utility in complex agentic tasks such as those posed by the Humanity‚Äôs Last Exam (HLE) [1]. Tool use is a promising avenue for the extension of their capabilities beyond what can be learned from the training data. By calling on external resources through search engines and code interpreters, tool use has been shown to enhance accuracy and reduce hallucinations [2, 3, 4, 5, 6, 7, 8, 9, 10].",
          "Prior research on tool-use agents has primarily focused on equipping a single powerful model with utility tools such as web search or calculators. While effective in many scenarios, this approach underutilizes the potential of tools: humans, when reasoning, routinely extend themselves by calling upon resources of greater-than-human intelligence, from domain experts to sophisticated processes and software systems. Motivated by this observation, we propose the orchestration paradigm. Under this paradigm, intelligence emerges not from a monolith but from a composite system. At the center of the system lies an orchestrator model, whose responsibility is to invoke the right tools for the given task, and to do so in the right order to accomplish the task. The crucial difference to the standard monolithic setup featuring a single powerful model is that in addition to deterministic utilities such as web search functions and code interpreters, models of various capabilities are made available to the orchestrator as intelligent tools. The use of tools of different levels of intelligence comes at varying costs, and the challenge for the orchestrator is then to dynamically decide on which tools to invoke in order to solve the task while respecting user preferences for various tools and minimizing the cost. By delegating narrowed-down sub-problems of a larger effort requiring intelligence to intelligent tools instead of handling the entire effort by a single generalist, orchestration teems with the promise of exhibiting higher intelligence than any of the system‚Äôs tools and leading monolithic solutions alike.",
          "One approach to implementing the orchestrator paradigm is to employ a language model as the orchestrator and allow it to invoke stronger models only when it deems it necessary. This can be done naively by prompting an off-the-shelf language model or by training a general-purpose orchestrator. For the former, we find that relying on straightforward model prompting is brittle and introduces systemic biases. As shown in Figure 3 (left and middle), GPT-5 disproportionately delegates tasks to GPT-5-mini, while Qwen3-8B defers to GPT-5 at a markedly higher rate. This illustrates two present issues of prompting in the context of complex tool orchestration: (i) the overuse of developmentally-related variants of oneself, i.e., self-enhancement bias [11], and (ii) defaulting to the strongest available tool regardless of the cost or relative utility (see Appendix A for more details and ¬ß4 for a thorough comparison to baselines). As such, we conclude that the scenarios in which an orchestrating model may call on models and tools of capabilities both inferior and superior to its own are idiosyncratic in the context of model tool calling and warrant their own approach to training. In addition, controllability in tool-use agents remains underexplored along two axes: cost‚Äìefficiency and user preferences (cf. ¬ß7).",
          "We address these shortcomings by proposing ToolOrchestra (shown in Figure 2), a novel method for training a small language model to act as the orchestrator ‚Äì the ‚Äúbrain‚Äù of a heterogeneous tool-use agent. Using ToolOrchestra, we produce the Orchestrator, an 8B-parameter model trained end-to-end with reinforcement learning (RL) to decide when and how to invoke more intelligent language models and various tools such as web search or code interpreters, and how to combine them in multi-turn reasoning. Our reward design balances three objectives ‚Äì correctness of the final outcome, efficiency in resource usage, and alignment with user preferences ‚Äì to yield a cost-effective and user-controllable tool-use policy. To aid RL training, we build an automatic data synthesis pipeline that generates thousands of verifiable multi-turn tool-use training examples with complex environments across 10 domains. We will make the resulting dataset, ToolScale, publicly available to facilitate further research on tool-use agent training.",
          "In our experiments, we rigorously evaluate the merits of our approach on three challenging tasks. On HLE [1], a benchmark consisting of difficult questions across many disciplines, we find that Orchestrator substantially outperforms prior methods with far lower computational cost. We also test on œÑ2\\tau^{2}-Bench [12], a function-calling benchmark, where Orchestrator demonstrates the ability to schedule a variety of tools effectively, calling a large model (GPT-5) in only ‚àº\\sim40% of the steps and utilizing cheaper models or tools for the rest, yet still exceeding the performance of an agent that uses the large model for every step. Finally, additional evaluations on the FRAMES [13], a factuality reasoning benchmark, provide further evidence of the versatility and robustness of our approach. We observe that even though the training and testing tasks differ markedly, the RL-trained Orchestrator adapts its tool-use policy to new challenges, indicating a high degree of general reasoning ability.",
          "Our contributions can be summarized as follows: (1) We introduce ToolOrchestra, a method for training a small language model to serve as the orchestrator of a diverse toolkit, including classical tools and more intelligent models. This dovetails with recent developments in the field testifying that small language models are often sufficiently powerful and far more economical in agentic systems [14, 15]. (2) We develop a novel reward training design that goes beyond accuracy. The resulting Orchestrator is trained end-to-end to balance task outcome correctness, efficiency in cost and latency, and alignment with user cost and tool preferences. (3) We demonstrate that Orchestrator trained by ToolOrchestra achieves state-of-the-art performance on challenging reasoning benchmarks, surpassing frontier models while using only a fraction of their compute and wall-clock time, and that it generalizes robustly to unseen tasks and tools."
        ],
        "subsections": []
      },
      {
        "title": "2 Agentic Problem Formulation",
        "level": 2,
        "paragraphs": [],
        "subsections": [
          {
            "title": "2.1 Task Formulation",
            "level": 3,
            "paragraphs": [
              "We investigate multi-turn tool-use agentic tasks and formalize them as a Markov Decision Process (MDP) ‚Ñ≥=(ùí∞,ùíÆ,ùíú,ùí™,ùíØ,ùíµ,r,œÅ,Œ≥)\\mathcal{M}=(\\mathcal{U},\\mathcal{S},\\mathcal{A},\\mathcal{O},\\mathcal{T},\\mathcal{Z},r,\\rho,\\gamma) following conventions similar to prior work [16, 17, 18]. We are given an instruction u‚ààùí∞u\\in\\mathcal{U}, user action preferences p=(0‚â§pa‚â§1‚Äã for ‚Äãa‚ààùíú)p=\\left(0\\leq p_{a}\\leq 1\\text{ for }a\\in\\mathcal{A}\\right), an initial state drawn from œÅ(‚ãÖ|u)\\rho(\\cdot\\,|\\,u), an initial observation o0‚ààùí™o_{0}\\in\\mathcal{O}, and the environment state space ùíÆ\\mathcal{S}. At step kk, the Orchestrator chooses an action ak‚ààùíúa_{k}\\in\\mathcal{A} according to a policy œÄŒ∏‚Äã(ak|hk)\\pi_{\\theta}(a_{k}\\,|\\,h_{k}) where hk=(u,o0,a0,o1,‚Ä¶,ak‚àí1,ok)h_{k}=(u,o_{0},a_{0},o_{1},\\dots,a_{k-1},o_{k}) is the interaction history. The environment transitions according to ùíØ‚Äã(sk+1|sk,ak)\\mathcal{T}(s_{k+1}\\,|\\,s_{k},a_{k}) and emits an observation ok+1‚àºùíµ(‚ãÖ|sk+1,ak)o_{k+1}\\sim\\mathcal{Z}(\\cdot\\,|\\,s_{k+1},a_{k}). The actions aia_{i} come at costs cic_{i} and operational latency lil_{i}, and the alignment of each action with user preferences is paip_{a_{i}}. After NN interaction steps, Orchestrator has traced the trajectory œÑ=hN\\tau=h_{N} and the environment provides a reward r‚Äã(œÑ)‚àà[0,1]r(\\tau)\\in[0,1] based on its correctness. Our goal is to maximize the correctness reward r‚Äã(œÑ)r(\\tau) and the overall user preference alignment ‚àëpai\\sum p_{a_{i}} while minimizing the total cost ‚àëci\\sum c_{i} and the aggregate latency ‚àëli\\sum l_{i}."
            ],
            "subsections": []
          },
          {
            "title": "2.2 Multi-Turn Rollout",
            "level": 3,
            "paragraphs": [
              "Given a user task, Orchestrator produces a solution via an iterative rollout that interleaves tool use with environment feedback to form a trajectory of turns. The rollout is initialized with a predefined system prompt and the question; the model (assistant role) then generates an initial step that ends with an EOS token. Each turn follows a reasoning‚Äìaction‚Äìobservation loop: (1) Chain-of-thought (reasoning). The Orchestrator analyzes the current state and plans the next action. (2) Tool call (action). Based on its reasoning, Orchestrator selects a tool from the available set (e.g., APIs, specialized models, code interpreters) and specifies parameters. (3) Tool response (observation). If a tool call is present, the tool-call block is extracted and executed by the environment; the resulting output is appended to the context under the user role and fed back to the model for the next turn. This process repeats until Orchestrator receives a termination signal from the environment or the rollout reaches a maximum of 50 turns."
            ],
            "subsections": []
          }
        ]
      },
      {
        "title": "3 ToolOrchestra",
        "level": 2,
        "paragraphs": [
          "Our approach, ToolOrchestra, centers on training a small language model as an intelligent agentic model capable of solving complex tasks by dynamically selecting and utilizing a wide variety of external tools. We hypothesize that small language models suffice for this purpose if they are taught to coordinate more intelligent tools strategically, and thus choose to train an 8B model. ToolOrchestra consists of an end-to-end reinforcement learning setup where the model under training, termed Orchestrator, learns to generate optimal multi-step reasoning and tool-use trajectories. The overall architecture is illustrated in Figure 2."
        ],
        "subsections": [
          {
            "title": "3.1 Unified Tool Calling",
            "level": 3,
            "paragraphs": [
              "In contrast to prior tool-use agents [19, 20], we broaden the toolset to include domain-specialized models and expose all tools through a single, unified interface. Tools are specified in JSON as a list of objects; each object defines the tool name, description, and a typed parameter schema (names and descriptions). When LLMs are used as tools, we obtain their descriptions with the following steps: (1). randomly sample 10 training tasks; (2). obtain the trajectories of LLMs to finish these tasks; (3). Ask another LLM to write the description based on the task instructions, LLM trajectories and whether LLMs complete the tasks. In Appendix C, we show an example description of Qwen3-32B. The complete catalog of tools used in our training is provided in Appendix D."
            ],
            "subsections": []
          },
          {
            "title": "3.2 End-to-End Agentic Reinforcement Learning",
            "level": 3,
            "paragraphs": [],
            "subsections": []
          },
          {
            "title": "3.3 Data Synthesis",
            "level": 3,
            "paragraphs": [],
            "subsections": []
          }
        ]
      },
      {
        "title": "4 Experimental Setting",
        "level": 2,
        "paragraphs": [],
        "subsections": [
          {
            "title": "4.1 Tools",
            "level": 3,
            "paragraphs": [
              "In the training, we prepare a large and comprehensive tool set (Appendix D), but only sample a subset for each training instance to build diverse tool configurations (¬ß3.3). We fix the following tool set in the evaluation for fair comparison.",
              "Basic tools. We use Tavily search API 111https://www.tavily.com/ for web search, Python sandbox for Code interpreter, and build Faiss index with Qwen3-Embedding-8B [22] for local search. Additionally, we also incorporate domain-specific functions, such as get_flight_status, to address specialized challenges within those domains.",
              "Specialized LLMs. We prompt GPT-5 [23], GPT-5-mini [23] as code writer, employ Qwen2.5-Coder-32B-Instruct [24] as another code writer, and leverage Qwen2.5-Math-72B [25], Qwen2.5-Math-7B [25] as specialized math models.",
              "Generalist LLMs. We consider GPT-5, GPT-5-mini, Llama-3.3-70B-Instruct [26], and Qwen3-32B [27] as representative generalist models."
            ],
            "subsections": []
          },
          {
            "title": "4.2 Baselines",
            "level": 3,
            "paragraphs": [
              "We compare Orchestrator-8B produced by ToolOrchestra to baseline orchestrators constructed by prompting LLMs. Additionally, we also compare to off-the-shelf monolithic LLM systems that are (1) not equipped with tools, (2) equipped with basic tools, and (3) using the expanded tool set that further includes specialized expert models and strong generalist models.",
              "For off-the-shelf LLMs, we evaluate GPT-5, Claude Opus 4.1 [28], Llama-3.3-70B-Instruct, Qwen3-235B-A22B [27], Llama-3_3-Nemotron-Super-49B-v1 [29], Qwen3-8B [27]."
            ],
            "subsections": []
          },
          {
            "title": "4.3 Evaluation Configuration",
            "level": 3,
            "paragraphs": [
              "We conduct experiments on three popular benchmarks with complex reasoning: Humanity‚Äôs Last Exam (HLE), FRAMES, and œÑ2\\tau^{2}-Bench. Details about these three benchmarks are given in Appendix B. Throughout the evaluation, we use the official price for proprietary models and leverage the pricing systems of TogetherAI222https://www.together.ai/pricing for open-source models. We set the inference temperature to 0 and allow maximum 50 turn for Orchestrator to solve a task."
            ],
            "subsections": []
          },
          {
            "title": "4.4 Training Configuration",
            "level": 3,
            "paragraphs": [
              "We employ Qwen3-8B as the backbone LLM and train it on the GeneralThought-430K 333https://huggingface.co/datasets/natolambert/GeneralThought-430K-filtered dataset in conjunction with synthetic data (¬ß\\S3.3). The training configuration uses a learning rate of 1e-6, a maximum input sequence length of 24,000, and a maximum generation length of 8,000, with a training batch size of 16 and a rollout batch size of 8. We allow maximum 50 turns for the Orchestrator to finish a task during rollout and use 16 NVIDIA H100 GPUs throughout the training."
            ],
            "subsections": []
          }
        ]
      },
      {
        "title": "5 Experimental Results",
        "level": 2,
        "paragraphs": [
          "We compare Orchestrator against a wide range of baselines across HLE, FRAMES, and œÑ2\\tau^{2}-Bench. The results are summarized in Table 1. For simple prompting methods without tools, models such as Qwen3-235B-A22B and Llama-3.3-70B fail to demonstrate strong performance. This highlights the inherent difficulty of the benchmarks, where tool use or additional reasoning mechanisms is essential. Providing tool access improves performance in some cases. For instance, Claude Opus 4.1 with tool usage improves from 11.7 to 19.8 in HLE, and from 58.2 to 63.5 in FRAMES, but at the expense of 2.8x costs and 4x latency. Smaller models like Qwen3-8B perform poorly (4.7 on HLE), indicating that basic tools alone are insufficient. Combining tools with specialized and generalist LLMs generally improves results ‚Äî Qwen3-235B-A22B, for example, rises from 14.0 to 32.8 on HLE and from 39.5 to 74.2 on FRAMES, but consumes more than 2 times of cost and latency. However, the gains are inconsistent across different models. GPT-5 using both tools and models suffers from performance drop due to inherent biases, often defaulting to GPT-5-mini (¬ß6.1).",
          "In contrast, our Orchestrator-8B achieves 37.1 on HLE and 76.3 on FRAMES, surpassing all baselines by a large margin. In œÑ2\\tau^{2}-Bench, Orchestrator-8B outperforms GPT-5 using basic tools by 2.5%, exhibiting strong function calling capabilities. Notably, compared to GPT-5 with tool use (35.1 on HLE) and Qwen3-235B-A22B with tool + model (32.8 on HLE), our approach delivers consistent improvements of +2 to +4.3 absolute points, while using only a small fraction of cost and time. These gains are particularly striking given that Orchestrator has only 8B parameters, but is capable of coordinating more intelligent models such as GPT-5, and achieves better performance with lower cost, which highlights the effectiveness of the orchestration strategy. Overall, the results clearly demonstrate the effectiveness of ToolOrchestra and the superiority of Orchestrator model in both performance and efficiency."
        ],
        "subsections": []
      },
      {
        "title": "6 Analysis",
        "level": 2,
        "paragraphs": [],
        "subsections": [
          {
            "title": "6.1 Tool Use Analysis",
            "level": 3,
            "paragraphs": [
              "Figure 5 shows the proportion of calls to each tool when various models serve as the orchestrator to solve a task. Instead of excessively invoking strong models and expensive tools, Orchestrator-8B learns to coordinate them more strategically. For example, in choosing between different models, Claude Opus 4.1 relies on GPT-5 most of the time, while making fewer calls to other models. In contrast, GPT-5 prefers to use GPT-5-mini. Orchestrator-8B learns to choose between various tools strategically, and achieves superior performance while using significantly lower costs."
            ],
            "subsections": []
          },
          {
            "title": "6.2 Cost Analysis",
            "level": 3,
            "paragraphs": [
              "To analyze the cost-effectiveness, we display the performance on HLE as a function of cost in Figure 6. We experiment with settings where the maximum number of 10, 20, 50 and 100 turns are allowed to finish a task. As the maximum number of allowed turns increases (i.e., cost increases), all models show improved performance. Orchestrator-8B consistently outperforms GPT-5, Claude Opus 4.1 and Qwen3-235B-A22B at a given budget, and can achieve similar results at a substantially lower cost. This demonstrates the capability of Orchestrator-8B to manage a heterogeneous set of tools, and pushes the intelligence boundary of the system as a whole."
            ],
            "subsections": []
          },
          {
            "title": "6.3 Generalization",
            "level": 3,
            "paragraphs": [
              "To evaluate Orchestrator-8B‚Äôs generalization capability, we test it with a tool configuration containing models unseen during training: (1) Query writer: Claude Opus 4.1, o3-mini and GPT-4o [30]; (2) Code writer: Claude Opus 4.1, Claude Sonnet 4.1 and Codestral-22B-v0.1 [31]; (3) Math model: OpenMath-Llama-2-70b [32], DeepSeek-Math-7b-Instruct [21]; (4) Generalist Models: Claude Opus 4.1, Claude Sonnet 4.1 and Gemma-3-27b-it [33].",
              "We keep the basic tools (web search, local search and code interpreter) as the same mentioned in ¬ß4.1 and generate model descriptions following the same procedures mentioned in section ¬ß3.1. Table 2 demonstrates that Orchestrator-8B shows strong skills in using models as tools. Even provided with a set of models not seen during training, Orchestrator successfully adapts to it by understanding their skills and weaknesses from model descriptions, and consistently achieves the best performance at the lowest cost across HLE, Frames and œÑ2\\tau^{2}-Bench. This confirms that the diverse tool configurations during training effectively enhance the generalization capabilities of Orchestrator-8B. In Appendix H, we conduct further experiments to evaluate Orchestrator-8B on pricing configurations unseen in training."
            ],
            "subsections": []
          },
          {
            "title": "6.4 User Preferences",
            "level": 3,
            "paragraphs": [
              "To assess Orchestrator-8B‚Äôs ability to adapt to heterogeneous user preferences at test time, we evaluate it on the Preference-aware test set described in ¬ß3.3, where we concatenate each question with an additional preference instruction. We evaluate the model preference adherence performance by calculating the preference-aware rewards defined in Appendix L. Table 3 shows that, even strong monolithic systems such as GPT-5 struggle to faithfully follow user preferences. In contrast, Orchestrator-8B exhibits remarkably better adherence to user preferences."
            ],
            "subsections": []
          }
        ]
      },
      {
        "title": "7 Related Work",
        "level": 2,
        "paragraphs": [],
        "subsections": [
          {
            "title": "7.1 From Tool Learning to Generalist Agents",
            "level": 3,
            "paragraphs": [
              "Tool learning underpins advanced reasoning in LLMs, as many tasks require external APIs, search engines, or code interpreters. Early work [3, 2, 6] used supervised fine-tuning (SFT) on tool-labeled data like GPT-4 generated dialogues. More recently, tool use has been framed as a sequential decision-making problem optimized by RL, with systems such as WebGPT [34], Search-R1 [20], ToRL [19], StepTool [7], SWiRL [8], Nemotron-Research-Tool-N1 [9], and ToolRL [10]. Building on this foundation, generalist agents like deep research agents [35, 36, 37, 38] autonomously discover, analyze, and synthesize across sources to produce analyst-level reports, aligning with the vision of compound AI systems [39, 40]. Recent open-source frameworks like SmolAgent [41], WebAgent [42, 43, 44], OWL [45], AutoAgent [46], and OAgent [47] extend this trend toward modular, robust, and accessible systems, highlighting the broader democratization of generalist agents."
            ],
            "subsections": []
          },
          {
            "title": "7.2 From Tool-Use Accuracy to Efficiency and Controllability",
            "level": 3,
            "paragraphs": [
              "Beyond correctness, recent work emphasizes efficiency and controllability, aiming to reduce computational costs and better align outputs with user preferences. Prompting-based methods like Self Divide-and-Conquer [48], Efficient Agents [49], and SMART [50] adaptively invoke tools or fine-tune usage, though they often depend on heavy prompt engineering or curated datasets. RL provides a more flexible alternative, where reward shaping balances accuracy, efficiency, and reliability. Advances include integrating auxiliary signals (e.g., response length, task difficulty)[51, 52, 53] and combining verifiable signals with human feedback[54]. A related direction is weak-to-strong generalization [55], which explores eliciting stronger models from weaker supervision. The most relevant work, OTC [56], improves efficiency by penalizing excessive calls while preserving accuracy. Unlike the prior work, our approach addresses the broader orchestration problem by using RL to coordinate diverse tools and models, balancing correctness, efficiency, and user preference for finer alignment and more robust deployment."
            ],
            "subsections": []
          }
        ]
      },
      {
        "title": "8 Conclusion",
        "level": 2,
        "paragraphs": [
          "In this work, we presented ToolOrchestra, a method for training a small orchestration model to unify diverse tools and specialized models. By training Orchestrator end-to-end with reinforcement learning, we showed that it can learn to plan adaptive tool-use strategies guided by both outcome quality, efficiency, and human preference rewards. This enables the agent to dynamically balance performance and cost, rather than relying on static heuristics or purely supervised approaches. To aid reinforcement learning, we also contribute a complex user-agent-tool synthetic dataset ToolScale. Our experiments on challenging benchmarks demonstrate that our Orchestrator-8B attains state-of-the-art performance while operating at significantly lower cost compared to larger models. Looking ahead, we envision more sophisticated recursive orchestrator systems to push the upper bound of intelligence but also to further enhance efficiency in solving increasingly complex agentic tasks."
        ],
        "subsections": []
      }
    ],
    "figures": [
      {
        "src": "https://ar5iv.labs.arxiv.org/html/2511.21689/assets/x1.png",
        "alt": "Refer to caption",
        "caption": "Figure 1: ToolOrchestra shows consistently strong performance on HLE, FRAMES, and œÑ2\\tau^{2}-Bench with superior cost efficiency.",
        "label": "S0.F1"
      },
      {
        "src": "https://ar5iv.labs.arxiv.org/html/2511.21689/assets/x3.png",
        "alt": "Refer to caption",
        "caption": "Figure 2: Overview of Orchestrator. Given a task, Orchestrator alternates between reasoning and tool calling in multiple turns to solve it. Orchestrator interacts with a diverse tool set, including basic tools (web search, functions such as get_flight_status, etc.), specialized LLMs (coding models, math models, etc.) and generalist LLMs (GPT-5, Claude Opus 4.1, etc.). In training under ToolOrchestra, Orchestrator is jointly optimized by outcome, efficiency and preference rewards via reinforcement learning.",
        "label": "S1.F2"
      },
      {
        "src": "https://ar5iv.labs.arxiv.org/html/2511.21689/assets/x4.png",
        "alt": "Refer to caption",
        "caption": "Figure 3: Tool-calling preferences exhibited by a prompted off-the-shelf or RL-trained model. GPT-5 tends to call GPT-5-mini most of the time, while Qwen3-8B relies heavily on GPT-5.",
        "label": "S1.F3"
      },
      {
        "src": "https://ar5iv.labs.arxiv.org/html/2511.21689/assets/x5.png",
        "alt": "Refer to caption",
        "caption": "Figure 4: Overview of ToolScale data synthesis pipeline. Starting from a domain, LLM will (1) firstly generate domain-specific database and tool APIs to simulate the environment and (2) then generate diverse user tasks together with their corresponding golden actions.",
        "label": "S3.F4"
      },
      {
        "src": "https://ar5iv.labs.arxiv.org/html/2511.21689/assets/x6.png",
        "alt": "Refer to caption",
        "caption": "Figure 5: The proportion of tool calls made by LLMs to solve a task (averaged across HLE, Frames and œÑ2\\tau^{2}-bench). Qwen-32B refers to Qwen3-32B [27] and Coder-32B refers to Qwen2.5-Coder-32B-Instruct [24]. Compared to other strong foundation models, Orchestrator-8B makes more balanced tool calls, and does not exhibit strong biases toward a particular tool or model. Detailed statistics are shown in Table 15.",
        "label": "S6.F5"
      },
      {
        "src": "https://ar5iv.labs.arxiv.org/html/2511.21689/assets/x7.png",
        "alt": "Refer to caption",
        "caption": "Figure 6: The relationship between performance and cost. Compared to strong monolithic LLM systems, Orchestrator (ours) achieves the best cost-effectiveness.",
        "label": "S6.F6"
      }
    ],
    "tables": [
      {
        "caption": "Table 1: Comparison of Orchestrator-8B with baselines (prompt-based LLMs). Llama-Nemotron-49B denotes Llama-3.3-Nemotron-Super-49B-v1. Cost in US cents, Latency in minutes, are averaged between HLE and Frames. More efficiency statistics on œÑ2\\tau^{2}-Bench are in Table 16 in Appendix. Basic tools include domain functions, search and code interpreter (¬ß4.1). ‚Üë\\uparrow The higher the better. ‚Üì\\downarrow The lower the better. The results of existing SOTA are reported by [23]‚Ä†.",
        "headers": [],
        "rows": [
          [
            "Tools",
            "Model(s)",
            "HLE (‚Üë\\uparrow)",
            "FRAMES (‚Üë\\uparrow)",
            "œÑ2\\tau^{2}-Bench (‚Üë\\uparrow)",
            "Cost (‚Üì\\downarrow)",
            "Latency (‚Üì\\downarrow)"
          ],
          [
            "Existing reported SOTA",
            "GPT-5",
            "35.2",
            "‚Äì",
            "84.2‚Ä°",
            "‚Äì",
            "‚Äì"
          ],
          [
            "o3",
            "24.3",
            "‚Äì",
            "68.4",
            "‚Äì",
            "‚Äì"
          ],
          [
            "GPT-4o",
            "5.3",
            "‚Äì",
            "43.8",
            "‚Äì",
            "‚Äì"
          ],
          [
            "No tool",
            "Qwen3-8B",
            "3.2",
            "24.2",
            "‚Äì‚àó",
            "0.2",
            "0.6"
          ],
          [
            "Llama-Nemotron-49B",
            "3.6",
            "25.6",
            "‚Äì‚àó",
            "0.4",
            "1.1"
          ],
          [
            "Llama-3.3-70B",
            "3.8",
            "32.4",
            "‚Äì‚àó",
            "0.5",
            "1.4"
          ],
          [
            "Qwen3-235B-A22B",
            "5.2",
            "34.3",
            "‚Äì‚àó",
            "2.6",
            "3.3"
          ],
          [
            "Claude Opus 4.1",
            "11.7",
            "58.2",
            "‚Äì‚àó",
            "27.4",
            "8.2"
          ],
          [
            "GPT-5",
            "23.4",
            "66.3",
            "‚Äì‚àó",
            "6.2",
            "4.1"
          ],
          [
            "Basic tools",
            "Qwen3-8B",
            "4.7",
            "26.5",
            "40.7",
            "1.3",
            "2.2"
          ],
          [
            "Llama-Nemotron-49B",
            "6.8",
            "28.2",
            "23.2",
            "2.5",
            "3.5"
          ],
          [
            "Llama-3.3-70B",
            "4.6",
            "42.3",
            "17.6",
            "2.8",
            "4.3"
          ],
          [
            "Qwen3-235B-A22B",
            "14.0",
            "39.5",
            "52.9",
            "12.3",
            "10.2"
          ],
          [
            "Claude Opus 4.1",
            "19.8",
            "63.5",
            "46.0",
            "76.2",
            "32.5"
          ],
          [
            "GPT-5",
            "35.1",
            "74.0",
            "77.7",
            "30.2",
            "19.8"
          ],
          [
            "Basic tools, Specialized LLMs Generalist LLMs",
            "Qwen3-8B",
            "30.6",
            "68.9",
            "72.3",
            "27.6",
            "18.3"
          ],
          [
            "Llama-Nemotron-49B",
            "25.8",
            "57.9",
            "66.7",
            "25.6",
            "17.1"
          ],
          [
            "Llama-3.3-70B",
            "19.7",
            "52.4",
            "55.8",
            "19.7",
            "13.4"
          ],
          [
            "Qwen3-235B-A22B",
            "32.8",
            "74.2",
            "75.6",
            "29.7",
            "21.2"
          ],
          [
            "Claude Opus 4.1",
            "34.6",
            "72.8",
            "76.8",
            "52.5",
            "25.6"
          ],
          [
            "GPT-5",
            "21.2",
            "57.5",
            "62.3",
            "17.8",
            "13.6"
          ],
          [
            "Orchestrator-8B",
            "37.1",
            "76.3",
            "80.2",
            "9.2",
            "8.2"
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 2: Generalization performance of Orchestrator-8B on HLE, Frames and œÑ2\\tau^{2}-Bench.",
        "headers": [
          "Model(s)",
          "HLE (‚Üë\\uparrow)",
          "Frames (‚Üë\\uparrow)",
          "œÑ2\\tau^{2}-Bench (‚Üë\\uparrow)",
          "Cost (‚Üì\\downarrow)",
          "Latency (‚Üì\\downarrow)"
        ],
        "rows": [
          [
            "12.6",
            "34.9",
            "38.3",
            "37.9",
            "10.6"
          ],
          [
            "13.9",
            "32.7",
            "22.9",
            "53.6",
            "8.3"
          ],
          [
            "13.2",
            "49.3",
            "12.8",
            "63.3",
            "10.1"
          ],
          [
            "14.7",
            "63.5",
            "38.7",
            "87.2",
            "13.8"
          ],
          [
            "17.8",
            "53.6",
            "43.4",
            "102.4",
            "19.5"
          ],
          [
            "16.4",
            "54.8",
            "44.8",
            "81.3",
            "14.6"
          ],
          [
            "22.0",
            "73.8",
            "48.8",
            "34.8",
            "8.2"
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 3: Preference performance comparison. The results show that Orchestrator-8B best adapts to user preference during test time.",
        "headers": [
          "Model(s)",
          "HLE",
          "Frames",
          "œÑ2\\tau^{2}-Bench"
        ],
        "rows": [
          [
            "25.3",
            "43.2",
            "55.7"
          ],
          [
            "27.6",
            "50.1",
            "56.9"
          ],
          [
            "22.3",
            "44.5",
            "55.4"
          ],
          [
            "37.9",
            "54.5",
            "68.2"
          ],
          [
            "40.2",
            "63.4",
            "73.5"
          ],
          [
            "34.6",
            "62.3",
            "70.3"
          ],
          [
            "46.7",
            "68.4",
            "79.5"
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 4: Generalization performance under different a pricing configuration. Orchestrator-8B consistently performs the best in terms of performance, cost and latency, which illustates the robustness of the Orchestrator",
        "headers": [],
        "rows": [
          [
            ""
          ],
          [
            "Qwen3-8B",
            "29.7",
            "68.2",
            "71.9",
            "27.4",
            "17.9"
          ],
          [
            "Nemotron-49B",
            "25.6",
            "57.8",
            "66.3",
            "24.3",
            "17.2"
          ],
          [
            "Llama-3.3-70B",
            "19.6",
            "52.2",
            "55.4",
            "17.9",
            "12.0"
          ],
          [
            "Qwen3-235B-A22B",
            "32.4",
            "74.1",
            "75.3",
            "27.9",
            "20.8"
          ],
          [
            "Claude Opus 4.1",
            "34.5",
            "72.3",
            "76.4",
            "52.3",
            "25.1"
          ],
          [
            "GPT-5",
            "20.8",
            "57.3",
            "61.9",
            "17.5",
            "13.2"
          ],
          [
            "Orchestrator-8B",
            "36.9",
            "76.6",
            "80.4",
            "7.5",
            "7.8"
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 5: Statistics of ToolScale: the number of tools, database entries, and tasks per domain.",
        "headers": [
          "",
          "Finanace",
          "Sport",
          "E-commerce",
          "Medicine",
          "Entertainment",
          "Railway",
          "Restaurant",
          "Education",
          "Travel",
          "Weather"
        ],
        "rows": [
          [
            "22",
            "19",
            "15",
            "19",
            "24",
            "25",
            "23",
            "21",
            "15",
            "14"
          ],
          [
            "686",
            "423",
            "577",
            "920",
            "852",
            "790",
            "683",
            "816",
            "752",
            "549"
          ],
          [
            "396",
            "247",
            "343",
            "622",
            "561",
            "414",
            "348",
            "426",
            "331",
            "375"
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 6: Model prompts to generate subjects in a domain",
        "headers": [],
        "rows": [
          [
            "Generate a list of major subjects in {domain}."
          ],
          [
            "Output using the following format:"
          ],
          [
            "```"
          ],
          [
            "[[subject1, subject2, ‚Ä¶]]"
          ],
          [
            "```"
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 7: Model prompts to generate schema in a domain",
        "headers": [],
        "rows": [
          [
            "```"
          ],
          [
            "{demo_schema}"
          ],
          [
            "```"
          ],
          [
            "Generate another schema of similar formats for {domain}."
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 8: Model prompts to generate database entry",
        "headers": [],
        "rows": [
          [
            "Schema"
          ],
          [
            "```"
          ],
          [
            "{schema}"
          ],
          [
            "```"
          ],
          [
            "Following the schema, write records in the subject {subject}. Make sure that the values align with the definitions in the schema and are consistent in different places. Use the following format to output:"
          ],
          [
            "```"
          ],
          [
            "{ ‚Äú‚Ä¶\": ‚Ä¶, ‚Äú‚Ä¶\": ‚Ä¶, }"
          ],
          [
            "```"
          ],
          [
            "Wrap the dictionary within ```."
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 9: Model prompts to validate database entry",
        "headers": [],
        "rows": [
          [
            "Schema"
          ],
          [
            "```"
          ],
          [
            "{schema}"
          ],
          [
            "```"
          ],
          [
            "Database entry"
          ],
          [
            "```"
          ],
          [
            "{db_entry}"
          ],
          [
            "```"
          ],
          [
            "Please check whether the following conditions are satisfied:"
          ],
          [
            "Condition 1. The database entry strictly aligns with the fields and type definitions in the schema."
          ],
          [
            "Condition 2. The values in the database entry are consistent across different places, e.g., id, name, etc."
          ],
          [
            "Condition 3. The database content is logical, natural, and reasonable."
          ],
          [
            "Output using the following format:"
          ],
          [
            "```"
          ],
          [
            "{ ‚Äúcondition 1\": ‚Äúsatisfied or not satisfied, ‚Äúcondition 2\": ‚Äúsatisfied or not satisfied, ‚Äúcondition 3\": ‚Äúsatisfied or not satisfied, }"
          ],
          [
            "```"
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 10: Model prompts to generate functions",
        "headers": [],
        "rows": [
          [
            "Demonstration function"
          ],
          [
            "```"
          ],
          [
            "{demo_function}"
          ],
          [
            "```"
          ],
          [
            "Following the formats of demonstration function, write frequently-used functions in {domain}. Wrap the functions within ```."
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 11: Model prompts to generate intents",
        "headers": [],
        "rows": [
          [
            "Functions"
          ],
          [
            "```"
          ],
          [
            "{functions}"
          ],
          [
            "```"
          ],
          [
            "Propose realistic intents in {domain} that could be solved by the functions above. Use the following format to output:"
          ],
          [
            "```."
          ],
          [
            "[["
          ],
          [
            "‚Äúpurpose 1\","
          ],
          [
            "‚Äúpurpose 2\","
          ],
          [
            "‚Ä¶"
          ],
          [
            "]]"
          ],
          [
            "```."
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 12: Model prompts to generate tasks",
        "headers": [],
        "rows": [
          [
            "Functions"
          ],
          [
            "```"
          ],
          [
            "{functions}"
          ],
          [
            "```"
          ],
          [
            "Database"
          ],
          [
            "```"
          ],
          [
            "{database}"
          ],
          [
            "```"
          ],
          [
            "Propose a realistic task with the intent: {intent}. Use the following format to output:"
          ],
          [
            "```."
          ],
          [
            "{task_template}"
          ],
          [
            "```."
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 13: Model prompts to evolve tasks",
        "headers": [],
        "rows": [
          [
            "Functions"
          ],
          [
            "```"
          ],
          [
            "{functions}"
          ],
          [
            "```"
          ],
          [
            "Database"
          ],
          [
            "```"
          ],
          [
            "{database}"
          ],
          [
            "```"
          ],
          [
            "Old task: {task}"
          ],
          [
            "Make a new task by adding more complexity to the old task. You can add constraints, involve more entities, make the situation more tricky, etc. Use the following format to output:"
          ],
          [
            "```."
          ],
          [
            "{task_template}"
          ],
          [
            "```."
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 14: Database schema example",
        "headers": [],
        "rows": [
          [
            "{"
          ],
          [
            "‚Äúmovies\": {"
          ],
          [
            "‚ÄúMMMMMMM\": { movie_id"
          ],
          [
            "‚Äúmovie_id\": \"MMMMMMM\","
          ],
          [
            "‚Äútitle\": \"‚Ä¶\","
          ],
          [
            "‚Äúgenres\": [[‚ÄúAction\", ‚ÄúAdventure\", ‚ÄúComedy\", ‚ÄúDrama\", ‚ÄúHorror\", ‚ÄúThriller\", ‚ÄúRomance\", ‚ÄúScience Fiction\", ‚ÄúFantasy\", ‚ÄúMystery\"]],"
          ],
          [
            "‚Äúruntime_minutes\": ‚Ä¶,"
          ],
          [
            "‚Äúmpaa_rating\": ‚Äú‚Ä¶\","
          ],
          [
            "‚Äúlanguages_audio\": [[\"‚Ä¶\"]],"
          ],
          [
            "‚Äúsubtitles\": [[\"‚Ä¶\"]],"
          ],
          [
            "‚Äúformats\": [[\"2D\", \"3D\", \"IMAX\", \"Dolby\"]],"
          ],
          [
            "‚Äúrelease_date\": ‚ÄúYY-MM-DD\","
          ],
          [
            "‚Äúend_of_run_est\": ‚ÄúYY-MM-DD\","
          ],
          [
            "‚Äúcast\": [["
          ],
          [
            "{ ‚Äúname\": ‚Äú‚Ä¶\", ‚Äúrole\": ‚Äú‚Ä¶\" }"
          ],
          [
            "]],"
          ],
          [
            "‚Äúcrew\": {"
          ],
          [
            "‚Äúdirector\": ‚Äú‚Ä¶\","
          ],
          [
            "‚Äúwriter\": ‚Äú‚Ä¶\","
          ],
          [
            "‚Äúproducer\": ‚Äú‚Ä¶\","
          ],
          [
            "‚Äúmusic\": ‚Äú‚Ä¶\""
          ],
          [
            "},"
          ],
          [
            "\"synopsis\": \"‚Ä¶\""
          ],
          [
            "},"
          ],
          [
            "‚Ä¶"
          ],
          [
            "},"
          ],
          [
            "‚Ä¶"
          ],
          [
            "}"
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 15: The average number of calls on each tool when various models serve as the orchestrator to solve an instance (averaged across HLE, Frames and œÑ2\\tau^{2}-bench). Qwen-32B refers to Qwen/Qwen3-32B [27], Coder-32B refers to Qwen/Qwen2.5-Coder-32B-Instruct [24], Math-7B refers to https://huggingface.co/Qwen/Qwen2.5-Math-7B-Instruct [25], Math-72B refers Qwen/Qwen2.5-Math-72B-Instruct [25], and Llama-70B refers to meta-llama/Llama-3.3-70B-Instruct [26]. Compared to other strong foundation models, Orchestrator-8B achieves better results (Table 1) while making few calls to GPT-5.",
        "headers": [
          "Model",
          "GPT-5",
          "GPT-5-mini",
          "Qwen-32B",
          "Coder-32B",
          "Math-72B",
          "Math-7B",
          "Llama-70B",
          "Local search",
          "Web search",
          "Code interpreter"
        ],
        "rows": [
          [
            "Qwen3-8B",
            "6.0",
            "0.5",
            "1.4",
            "0.5",
            "0.0",
            "0.0",
            "0.0",
            "0.8",
            "1.2",
            "1.6"
          ],
          [
            "Nemontron-49B",
            "5.1",
            "1.6",
            "0.5",
            "0.8",
            "0.1",
            "0.1",
            "0.3",
            "0.7",
            "0.9",
            "1.4"
          ],
          [
            "Llama-3.3-70B",
            "1.8",
            "0.0",
            "0.1",
            "0.0",
            "1.4",
            "0.3",
            "4.8",
            "0.6",
            "1.4",
            "1.3"
          ],
          [
            "Qwen3-235B-A22B",
            "6.2",
            "0.3",
            "0.6",
            "1.2",
            "0.6",
            "0.1",
            "1.1",
            "1.4",
            "1.0",
            "2.2"
          ],
          [
            "Claude Opus 4.1",
            "6.2",
            "0.2",
            "0.3",
            "0.3",
            "0.1",
            "0.0",
            "0.1",
            "1.0",
            "1.3",
            "1.4"
          ],
          [
            "GPT-5",
            "2.7",
            "5.6",
            "0.0",
            "0.2",
            "0.0",
            "0.0",
            "0.0",
            "0.5",
            "0.7",
            "1.0"
          ],
          [
            "Orchestrator-8B",
            "1.6",
            "1.7",
            "1.3",
            "0.2",
            "0.0",
            "0.1",
            "0.0",
            "1.8",
            "0.7",
            "0.8"
          ]
        ],
        "label": null
      },
      {
        "caption": "Table 16: The cost and latency of LLMs in œÑ2\\tau^{2}-Bench. Orchestrator-8B consistently shows better performance with lower cost and latency.",
        "headers": [],
        "rows": [
          [
            "Tools",
            "Model(s)",
            "œÑ2\\tau^{2}-Bench (‚Üë\\uparrow)",
            "Cost (‚Üì\\downarrow)",
            "Latency (‚Üì\\downarrow)"
          ],
          [
            "Basic tools",
            "Qwen3-8B",
            "40.7",
            "1.6",
            "2.3"
          ],
          [
            "Llama-Nemotron-49B",
            "23.2",
            "2.7",
            "3.6"
          ],
          [
            "Llama-3.3-70B",
            "17.6",
            "3.1",
            "4.5"
          ],
          [
            "Qwen3-235B-A22B",
            "52.9",
            "12.6",
            "10.6"
          ],
          [
            "Claude Opus 4.1",
            "46.0",
            "81.2",
            "32.8"
          ],
          [
            "GPT-5",
            "77.7",
            "31.3",
            "20.2"
          ],
          [
            "Basic tools, Specialized LLMs Generalist LLMs",
            "Qwen3-8B",
            "72.3",
            "27.9",
            "18.4"
          ],
          [
            "Llama-Nemotron-49B",
            "66.7",
            "25.8",
            "17.5"
          ],
          [
            "Llama-3.3-70B",
            "55.8",
            "20.1",
            "14.2"
          ],
          [
            "Qwen3-235B-A22B",
            "75.6",
            "30.0",
            "22.6"
          ],
          [
            "Claude Opus 4.1",
            "76.8",
            "52.8",
            "24.1"
          ],
          [
            "GPT-5",
            "62.3",
            "18.2",
            "14.5"
          ],
          [
            "Orchestrator-8B",
            "80.2",
            "10.3",
            "8.6"
          ]
        ],
        "label": null
      }
    ],
    "equations": [
      {
        "latex": "\\small r_{\\text{outcome}}(\\tau)=\\begin{cases}1&\\text{if }\\text{solved}(\\tau),\\\\\n0&\\text{otherwise}.\\end{cases}",
        "mathml": "<math alttext=\"\\small r_{\\text{outcome}}(\\tau)=\\begin{cases}1&amp;\\text{if }\\text{solved}(\\tau),\\\\\n0&amp;\\text{otherwise}.\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"S3.E1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi mathsize=\"0.900em\">r</mi><mtext mathsize=\"0.900em\">outcome</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">œÑ</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mn mathsize=\"0.900em\">1</mn></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mrow><mrow><mtext mathsize=\"0.900em\">if¬†</mtext><mtext mathsize=\"0.900em\">solved</mtext></mrow><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">œÑ</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><",
        "label": "(1)"
      },
      {
        "latex": "\\small r_{\\text{outcome}}(\\tau)=\\begin{cases}1&\\text{if }\\text{solved}(\\tau),\\\\\n0&\\text{otherwise}.\\end{cases}",
        "mathml": "<math alttext=\"\\small r_{\\text{outcome}}(\\tau)=\\begin{cases}1&amp;\\text{if }\\text{solved}(\\tau),\\\\\n0&amp;\\text{otherwise}.\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"S3.E1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi mathsize=\"0.900em\">r</mi><mtext mathsize=\"0.900em\">outcome</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">œÑ</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mn mathsize=\"0.900em\">1</mn></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mrow><mrow><mtext mathsize=\"0.900em\">if¬†</mtext><mtext mathsize=\"0.900em\">solved</mtext></mrow><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">œÑ</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><",
        "label": "(1)"
      },
      {
        "latex": "\\small R(\\tau)=\\begin{cases}M_{\\text{normalized}}^{\\tau}\\cdot P&\\text{if }r_{\\text{outcome}}(\\tau)\\\\\n0&\\text{otherwise}.\\end{cases}",
        "mathml": "<math alttext=\"\\small R(\\tau)=\\begin{cases}M_{\\text{normalized}}^{\\tau}\\cdot P&amp;\\text{if }r_{\\text{outcome}}(\\tau)\\\\\n0&amp;\\text{otherwise}.\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"S3.E2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathsize=\"0.900em\">R</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">œÑ</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msubsup><mi mathsize=\"0.900em\">M</mi><mtext mathsize=\"0.900em\">normalized</mtext><mi mathsize=\"0.900em\">œÑ</mi></msubsup><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">‚ãÖ</mo><mi mathsize=\"0.900em\">P</mi></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mtext mathsize=\"0.900em\">if¬†</mtext><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><msub><mi mathsize=\"0.900em\">r",
        "label": "(2)"
      },
      {
        "latex": "\\small R(\\tau)=\\begin{cases}M_{\\text{normalized}}^{\\tau}\\cdot P&\\text{if }r_{\\text{outcome}}(\\tau)\\\\\n0&\\text{otherwise}.\\end{cases}",
        "mathml": "<math alttext=\"\\small R(\\tau)=\\begin{cases}M_{\\text{normalized}}^{\\tau}\\cdot P&amp;\\text{if }r_{\\text{outcome}}(\\tau)\\\\\n0&amp;\\text{otherwise}.\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"S3.E2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathsize=\"0.900em\">R</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">œÑ</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msubsup><mi mathsize=\"0.900em\">M</mi><mtext mathsize=\"0.900em\">normalized</mtext><mi mathsize=\"0.900em\">œÑ</mi></msubsup><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">‚ãÖ</mo><mi mathsize=\"0.900em\">P</mi></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mtext mathsize=\"0.900em\">if¬†</mtext><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><msub><mi mathsize=\"0.900em\">r",
        "label": "(2)"
      },
      {
        "latex": "\\small A(\\tau)=\\frac{R(\\tau)-\\text{mean}_{\\tau\\in\\mathrm{T}}{R(\\tau)}}{\\text{std}_{\\tau\\in\\mathrm{T}}{R(\\tau)}}.",
        "mathml": "<math alttext=\"\\small A(\\tau)=\\frac{R(\\tau)-\\text{mean}_{\\tau\\in\\mathrm{T}}{R(\\tau)}}{\\text{std}_{\\tau\\in\\mathrm{T}}{R(\\tau)}}.\" class=\"ltx_Math\" display=\"block\" id=\"S3.E3.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi mathsize=\"0.900em\">A</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">œÑ</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">=</mo><mfrac><mrow><mrow><mi mathsize=\"0.900em\">R</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">œÑ</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">‚àí</mo><mrow><msub><mtext mathsize=\"0.900em\">mean</mtext><mrow><mi mathsize=\"0.900em\">œÑ</mi><mo mathsize=\"0.900em\">‚àà</mo><mi mathsize=\"0.900em\" mathvariant=\"normal\">T</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mi mathsize=\"0.900em\">R</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo",
        "label": "(3)"
      },
      {
        "latex": "\\small A(\\tau)=\\frac{R(\\tau)-\\text{mean}_{\\tau\\in\\mathrm{T}}{R(\\tau)}}{\\text{std}_{\\tau\\in\\mathrm{T}}{R(\\tau)}}.",
        "mathml": "<math alttext=\"\\small A(\\tau)=\\frac{R(\\tau)-\\text{mean}_{\\tau\\in\\mathrm{T}}{R(\\tau)}}{\\text{std}_{\\tau\\in\\mathrm{T}}{R(\\tau)}}.\" class=\"ltx_Math\" display=\"block\" id=\"S3.E3.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi mathsize=\"0.900em\">A</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">œÑ</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">=</mo><mfrac><mrow><mrow><mi mathsize=\"0.900em\">R</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">œÑ</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">‚àí</mo><mrow><msub><mtext mathsize=\"0.900em\">mean</mtext><mrow><mi mathsize=\"0.900em\">œÑ</mi><mo mathsize=\"0.900em\">‚àà</mo><mi mathsize=\"0.900em\" mathvariant=\"normal\">T</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mi mathsize=\"0.900em\">R</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo",
        "label": "(3)"
      },
      {
        "latex": "\\small\\mathcal{L}_{\\text{GRPO}}(\\theta)=\\mathbb{E}{\\tau\\sim\\pi_{\\theta}}\\Bigg[\\min\\Big(\\text{ratio}_{\\theta}(\\tau)A(\\tau),\\text{clip}(\\text{ratio}_{\\theta}(\\tau),1-\\epsilon,1+\\epsilon)A(\\tau)\\Big)\\Bigg],",
        "mathml": "<math alttext=\"\\small\\mathcal{L}_{\\text{GRPO}}(\\theta)=\\mathbb{E}{\\tau\\sim\\pi_{\\theta}}\\Bigg[\\min\\Big(\\text{ratio}_{\\theta}(\\tau)A(\\tau),\\text{clip}(\\text{ratio}_{\\theta}(\\tau),1-\\epsilon,1+\\epsilon)A(\\tau)\\Big)\\Bigg],\" class=\"ltx_Math\" display=\"block\" id=\"S3.E4.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">‚Ñí</mi><mtext mathsize=\"0.900em\">GRPO</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">Œ∏</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">=</mo><mrow><mi mathsize=\"0.900em\">ùîº</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mi mathsize=\"0.900em\">œÑ</mi></mrow><mo mathsize=\"0.900em\">‚àº</mo><mrow><msub><mi mathsize=\"0.900em\">œÄ</mi><mi mathsize=\"0.900em\">Œ∏</mi></msub><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"2.600em\" minsize=\"2.600em\">[</mo><mrow><mi mathsize=\"0.900em\">min</mi><mo>‚Å°</mo><mrow><mo maxsize=\"",
        "label": "(4)"
      },
      {
        "latex": "\\small\\mathcal{L}_{\\text{GRPO}}(\\theta)=\\mathbb{E}{\\tau\\sim\\pi_{\\theta}}\\Bigg[\\min\\Big(\\text{ratio}_{\\theta}(\\tau)A(\\tau),\\text{clip}(\\text{ratio}_{\\theta}(\\tau),1-\\epsilon,1+\\epsilon)A(\\tau)\\Big)\\Bigg],",
        "mathml": "<math alttext=\"\\small\\mathcal{L}_{\\text{GRPO}}(\\theta)=\\mathbb{E}{\\tau\\sim\\pi_{\\theta}}\\Bigg[\\min\\Big(\\text{ratio}_{\\theta}(\\tau)A(\\tau),\\text{clip}(\\text{ratio}_{\\theta}(\\tau),1-\\epsilon,1+\\epsilon)A(\\tau)\\Big)\\Bigg],\" class=\"ltx_Math\" display=\"block\" id=\"S3.E4.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">‚Ñí</mi><mtext mathsize=\"0.900em\">GRPO</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">Œ∏</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">=</mo><mrow><mi mathsize=\"0.900em\">ùîº</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mi mathsize=\"0.900em\">œÑ</mi></mrow><mo mathsize=\"0.900em\">‚àº</mo><mrow><msub><mi mathsize=\"0.900em\">œÄ</mi><mi mathsize=\"0.900em\">Œ∏</mi></msub><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"2.600em\" minsize=\"2.600em\">[</mo><mrow><mi mathsize=\"0.900em\">min</mi><mo>‚Å°</mo><mrow><mo maxsize=\"",
        "label": "(4)"
      },
      {
        "latex": "M^{{\\tau(e)}}_{\\text{normalized},s}[k]=\\begin{cases}M^{\\tau(e)}_{s}[k]/max(1,M^{\\tau(e)}_{0}[k])&\\text{if }1\\leq k\\leq n+1\\\\\nM^{\\tau(e)}_{0}[k]/max(1,M^{\\tau(e)}_{s}[k])&\\text{otherwise}.\\end{cases}",
        "mathml": "<math alttext=\"M^{{\\tau(e)}}_{\\text{normalized},s}[k]=\\begin{cases}M^{\\tau(e)}_{s}[k]/max(1,M^{\\tau(e)}_{0}[k])&amp;\\text{if }1\\leq k\\leq n+1\\\\\nM^{\\tau(e)}_{0}[k]/max(1,M^{\\tau(e)}_{s}[k])&amp;\\text{otherwise}.\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"A12.E5.m1\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>M</mi><mrow><mtext>normalized</mtext><mo>,</mo><mi>s</mi></mrow><mrow><mi>œÑ</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mrow><mrow><msubsup><mi>M</mi><mi>s</mi><mrow><mi>œÑ</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msubsup><mo lspac",
        "label": "(5)"
      },
      {
        "latex": "M^{{\\tau(e)}}_{\\text{normalized},s}[k]=\\begin{cases}M^{\\tau(e)}_{s}[k]/max(1,M^{\\tau(e)}_{0}[k])&\\text{if }1\\leq k\\leq n+1\\\\\nM^{\\tau(e)}_{0}[k]/max(1,M^{\\tau(e)}_{s}[k])&\\text{otherwise}.\\end{cases}",
        "mathml": "<math alttext=\"M^{{\\tau(e)}}_{\\text{normalized},s}[k]=\\begin{cases}M^{\\tau(e)}_{s}[k]/max(1,M^{\\tau(e)}_{0}[k])&amp;\\text{if }1\\leq k\\leq n+1\\\\\nM^{\\tau(e)}_{0}[k]/max(1,M^{\\tau(e)}_{s}[k])&amp;\\text{otherwise}.\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"A12.E5.m1\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi>M</mi><mrow><mtext>normalized</mtext><mo>,</mo><mi>s</mi></mrow><mrow><mi>œÑ</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msubsup><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo stretchy=\"false\">[</mo><mi>k</mi><mo stretchy=\"false\">]</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mrow><mrow><msubsup><mi>M</mi><mi>s</mi><mrow><mi>œÑ</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></mrow></msubsup><mo lspac",
        "label": "(5)"
      },
      {
        "latex": "\\small R^{e}(\\tau)=\\begin{cases}M^{{\\tau(e)}}_{\\text{normalized},s}\\cdot P&\\text{if }r_{\\text{outcome}(\\tau)}\\\\\n0&\\text{otherwise}.\\end{cases}",
        "mathml": "<math alttext=\"\\small R^{e}(\\tau)=\\begin{cases}M^{{\\tau(e)}}_{\\text{normalized},s}\\cdot P&amp;\\text{if }r_{\\text{outcome}(\\tau)}\\\\\n0&amp;\\text{otherwise}.\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"A12.E6.m1\" intent=\":literal\"><semantics><mrow><mrow><msup><mi mathsize=\"0.900em\">R</mi><mi mathsize=\"0.900em\">e</mi></msup><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">œÑ</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msubsup><mi mathsize=\"0.900em\">M</mi><mrow><mtext mathsize=\"0.900em\">normalized</mtext><mo mathsize=\"0.900em\">,</mo><mi mathsize=\"0.900em\">s</mi></mrow><mrow><mi mathsize=\"0.900em\">œÑ</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">e</mi><mo maxsize=\"0.900em\" m",
        "label": "(6)"
      },
      {
        "latex": "\\small R^{e}(\\tau)=\\begin{cases}M^{{\\tau(e)}}_{\\text{normalized},s}\\cdot P&\\text{if }r_{\\text{outcome}(\\tau)}\\\\\n0&\\text{otherwise}.\\end{cases}",
        "mathml": "<math alttext=\"\\small R^{e}(\\tau)=\\begin{cases}M^{{\\tau(e)}}_{\\text{normalized},s}\\cdot P&amp;\\text{if }r_{\\text{outcome}(\\tau)}\\\\\n0&amp;\\text{otherwise}.\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"A12.E6.m1\" intent=\":literal\"><semantics><mrow><mrow><msup><mi mathsize=\"0.900em\">R</mi><mi mathsize=\"0.900em\">e</mi></msup><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">œÑ</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><msubsup><mi mathsize=\"0.900em\">M</mi><mrow><mtext mathsize=\"0.900em\">normalized</mtext><mo mathsize=\"0.900em\">,</mo><mi mathsize=\"0.900em\">s</mi></mrow><mrow><mi mathsize=\"0.900em\">œÑ</mi><mo lspace=\"0em\" rspace=\"0em\">‚Äã</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">e</mi><mo maxsize=\"0.900em\" m",
        "label": "(6)"
      }
    ],
    "references": [
      "Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanity‚Äôs last exam. arXiv preprint arXiv:2501.14249, 2025.",
      "Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. In The Twelfth International Conference on Learning Representations, 2023.",
      "Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:68539‚Äì68551, 2023.",
      "Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei Huang, Chaojun Xiao, et al. Tool learning with foundation models. ACM Computing Surveys, 57(4):1‚Äì40, 2024.",
      "Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning. arXiv preprint arXiv:2410.02089, 2024.",
      "Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong, Zhong Zhang, Jie Zhou, Yankai Lin, Zhiyuan Liu, et al. Tell me more! towards implicit user intention understanding of language model driven agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1088‚Äì1113, 2024.",
      "Yuanqing Yu, Zhefan Wang, Weizhi Ma, Shuai Wang, Chuhan Wu, Zhiqiang Guo, and Min Zhang. Steptool: Enhancing multi-step tool usage in llms through step-grained reinforcement learning. arXiv preprint arXiv:2410.07745, 2024.",
      "Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, and Christopher D Manning. Synthetic data generation & multi-step rl for reasoning & tool use. arXiv preprint arXiv:2504.04736, 2025.",
      "Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and Guilin Liu. Nemotron-research-tool-n1: Exploring tool-using language models with reinforced reasoning. arXiv preprint arXiv:2505.00024, 2025.",
      "Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-T√ºr, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025.",
      "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:46595‚Äì46623, 2023.",
      "Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. œÑ2{\\tau}^{2}-Bench: Evaluating Conversational Agents in a Dual-Control Environment. arXiv preprint arXiv:2506.07982, 2025.",
      "Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation, 2024.",
      "Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, and Pavlo Molchanov. Small language models are the future of agentic ai. arXiv preprint arXiv:2506.02153, 2025.",
      "Bingxi Zhao, Lin Geng Foo, Ping Hu, Christian Theobalt, Hossein Rahmani, and Jun Liu. Llm-based agentic reasoning frameworks: A survey from methods to scenarios. arXiv preprint arXiv:2508.17692, 2025.",
      "Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, et al. Agentgym: Evolving large language model-based agents across diverse environments. arXiv preprint arXiv:2406.04151, 2024.",
      "Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.",
      "Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, et al. Agentgym-rl: Training llm agents for long-horizon decision making through multi-turn reinforcement learning. arXiv preprint arXiv:2509.08755, 2025.",
      "Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025.",
      "Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025.",
      "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.",
      "Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025.",
      "OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/. Accessed: 2025-09-23.",
      "Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024.",
      "An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024.",
      "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv‚Äì2407, 2024.",
      "An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.",
      "Anthropic. Claude opus 4.1. https://www.anthropic.com/news/claude-opus-4-1, 2025.",
      "Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-nemotron: Efficient reasoning models. arXiv preprint arXiv:2505.00949, 2025.",
      "OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/. Accessed: 2025-09-23.",
      "Mistral AI team. Codestral. https://mistral.ai/news/codestral, 2024.",
      "Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. Openmathinstruct-1: A 1.8 million math instruction tuning dataset. arXiv preprint arXiv: Arxiv-2402.10176, 2024.",
      "Team Google, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ram√©, Morgane Rivi√®re, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025.",
      "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.",
      "OpenAI. Introducing deep research, 2025.",
      "Google DeepMind. Gemini deep research ‚Äî your personal research assistant, 2025.",
      "Perplexity AI. Introducing perplexity deep research, 2025.",
      "Moonshot AI. Kimi-researcher: End-to-end rl training for emerging agentic capabilities, 2025.",
      "Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather Miller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao, and Ali Ghodsi. The shift from models to compound ai systems. https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/, 2024.",
      "Gohar Irfan Chaudhry, Esha Choukse, √ç√±igo Goiri, Rodrigo Fonseca, Adam Belay, and Ricardo Bianchini. Towards resource-efficient compound ai systems. In Proceedings of the 2025 Workshop on Hot Topics in Operating Systems, pages 218‚Äì224, 2025.",
      "Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik Kaunism√§ki. ‚Äòsmolagents‚Äò: a smol library to build great agentic systems. https://github.com/huggingface/smolagents, 2025.",
      "Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025.",
      "Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025.",
      "Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, et al. Webshaper: Agentically data synthesizing via information-seeking formalization. arXiv preprint arXiv:2507.15061, 2025.",
      "Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, et al. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation. arXiv preprint arXiv:2505.23885, 2025.",
      "Jiabin Tang, Tianyu Fan, and Chao Huang. Autoagent: A fully-automated and zero-code framework for llm agents. arXiv preprint arXiv:2502.05957, 2025.",
      "He Zhu, Tianrui Qin, King Zhu, Heyuan Huang, Yeyi Guan, Jinxiang Xia, Yi Yao, Hanhao Li, Ningning Wang, Pai Liu, et al. Oagents: An empirical study of building effective agents. arXiv preprint arXiv:2506.15741, 2025.",
      "Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxiang Wang, Huimin Wang, Guanhua Chen, and Kam-Fai Wong. Self-dc: When to reason and when to act? self divide-and-conquer for compositional unknown questions. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 6510‚Äì6525, 2025.",
      "Ningning Wang, Xavier Hu, Pai Liu, He Zhu, Yue Hou, Heyuan Huang, Shengyu Zhang, Jian Yang, Jiaheng Liu, Ge Zhang, et al. Efficient agents: Building effective agents while reducing cost. arXiv preprint arXiv:2508.02694, 2025.",
      "Cheng Qian, Emre Can Acikgoz, Hongru Wang, Xiusi Chen, Avirup Sil, Dilek Hakkani-T√ºr, Gokhan Tur, and Heng Ji. Smart: Self-aware agent for tool overuse mitigation. arXiv preprint arXiv:2502.11435, 2025.",
      "Pranjal Aggarwal and Sean Welleck. L1: Controlling how long a reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025.",
      "Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025.",
      "Rui Wang, Hongru Wang, Boyang Xue, Jianhui Pang, Shudong Liu, Yi Chen, Jiahao Qiu, Derek Fai Wong, Heng Ji, and Kam-Fai Wong. Harnessing the reasoning economy: A survey of efficient reasoning for large language models. arXiv preprint arXiv:2503.24377, 2025.",
      "Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, and Juanzi Li. Agentic reward modeling: Integrating human preferences with verifiable correctness signals for reliable reward systems. arXiv preprint arXiv:2502.19328, 2025.",
      "Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In International Conference on Machine Learning, pages 4971‚Äì5012. PMLR, 2024.",
      "Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, and Heng Ji. Otc: Optimal tool calls via reinforcement learning. arXiv e-prints, pages arXiv‚Äì2504, 2025.",
      "Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.",
      "Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025.",
      "Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024."
    ],
    "full_text": "## 1 Introduction\n\nLarge language models (LLMs) have been reported to have made remarkable strides towards superhuman intelligence but remain of limited utility in complex agentic tasks such as those posed by the Humanity‚Äôs Last Exam (HLE) [1]. Tool use is a promising avenue for the extension of their capabilities beyond what can be learned from the training data. By calling on external resources through search engines and code interpreters, tool use has been shown to enhance accuracy and reduce hallucinations [2, 3, 4, 5, 6, 7, 8, 9, 10].\n\nPrior research on tool-use agents has primarily focused on equipping a single powerful model with utility tools such as web search or calculators. While effective in many scenarios, this approach underutilizes the potential of tools: humans, when reasoning, routinely extend themselves by calling upon resources of greater-than-human intelligence, from domain experts to sophisticated processes and software systems. Motivated by this observation, we propose the orchestration paradigm. Under this paradigm, intelligence emerges not from a monolith but from a composite system. At the center of the system lies an orchestrator model, whose responsibility is to invoke the right tools for the given task, and to do so in the right order to accomplish the task. The crucial difference to the standard monolithic setup featuring a single powerful model is that in addition to deterministic utilities such as web search functions and code interpreters, models of various capabilities are made available to the orchestrator as intelligent tools. The use of tools of different levels of intelligence comes at varying costs, and the challenge for the orchestrator is then to dynamically decide on which tools to invoke in order to solve the task while respecting user preferences for various tools and minimizing the cost. By delegating narrowed-down sub-problems of a larger effort requiring intelligence to intelligent tools instead of handling the entire effort by a single generalist, orchestration teems with the promise of exhibiting higher intelligence than any of the system‚Äôs tools and leading monolithic solutions alike.\n\nOne approach to implementing the orchestrator paradigm is to employ a language model as the orchestrator and allow it to invoke stronger models only when it deems it necessary. This can be done naively by prompting an off-the-shelf language model or by training a general-purpose orchestrator. For the former, we find that relying on straightforward model prompting is brittle and introduces systemic biases. As shown in Figure 3 (left and middle), GPT-5 disproportionately delegates tasks to GPT-5-mini, while Qwen3-8B defers to GPT-5 at a markedly higher rate. This illustrates two present issues of prompting in the context of complex tool orchestration: (i) the overuse of developmentally-related variants of oneself, i.e., self-enhancement bias [11], and (ii) defaulting to the strongest available tool regardless of the cost or relative utility (see Appendix A for more details and ¬ß4 for a thorough comparison to baselines). As such, we conclude that the scenarios in which an orchestrating model may call on models and tools of capabilities both inferior and superior to its own are idiosyncratic in the context of model tool calling and warrant their own approach to training. In addition, controllability in tool-use agents remains underexplored along two axes: cost‚Äìefficiency and user preferences (cf. ¬ß7).\n\nWe address these shortcomings by proposing ToolOrchestra (shown in Figure 2), a novel method for training a small language model to act as the orchestrator ‚Äì the ‚Äúbrain‚Äù of a heterogeneous tool-use agent. Using ToolOrchestra, we produce the Orchestrator, an 8B-parameter model trained end-to-end with reinforcement learning (RL) to decide when and how to invoke more intelligent language models and various tools such as web search or code interpreters, and how to combine them in multi-turn reasoning. Our reward design balances three objectives ‚Äì correctness of the final outcome, efficiency in resource usage, and alignment with user preferences ‚Äì to yield a cost-effective and user-controllable tool-use policy. To aid RL training, we build an automatic data synthesis pipeline that generates thousands of verifiable multi-turn tool-use training examples with complex environments across 10 domains. We will make the resulting dataset, ToolScale, publicly available to facilitate further research on tool-use agent training.\n\nIn our experiments, we rigorously evaluate the merits of our approach on three challenging tasks. On HLE [1], a benchmark consisting of difficult questions across many disciplines, we find that Orchestrator substantially outperforms prior methods with far lower computational cost. We also test on œÑ2\\tau^{2}-Bench [12], a function-calling benchmark, where Orchestrator demonstrates the ability to schedule a variety of tools effectively, calling a large model (GPT-5) in only ‚àº\\sim40% of the steps and utilizing cheaper models or tools for the rest, yet still exceeding the performance of an agent that uses the large model for every step. Finally, additional evaluations on the FRAMES [13], a factuality reasoning benchmark, provide further evidence of the versatility and robustness of our approach. We observe that even though the training and testing tasks differ markedly, the RL-trained Orchestrator adapts its tool-use policy to new challenges, indicating a high degree of general reasoning ability.\n\nOur contributions can be summarized as follows: (1) We introduce ToolOrchestra, a method for training a small language model to serve as the orchestrator of a diverse toolkit, including classical tools and more intelligent models. This dovetails with recent developments in the field testifying that small language models are often sufficiently powerful and far more economical in agentic systems [14, 15]. (2) We develop a novel reward training design that goes beyond accuracy. The resulting Orchestrator is trained end-to-end to balance task outcome correctness, efficiency in cost and latency, and alignment with user cost and tool preferences. (3) We demonstrate that Orchestrator trained by ToolOrchestra achieves state-of-the-art performance on challenging reasoning benchmarks, surpassing frontier models while using only a fraction of their compute and wall-clock time, and that it generalizes robustly to unseen tasks and tools.\n\n## 2 Agentic Problem Formulation\n\n### 2.1 Task Formulation\n\nWe investigate multi-turn tool-use agentic tasks and formalize them as a Markov Decision Process (MDP) ‚Ñ≥=(ùí∞,ùíÆ,ùíú,ùí™,ùíØ,ùíµ,r,œÅ,Œ≥)\\mathcal{M}=(\\mathcal{U},\\mathcal{S},\\mathcal{A},\\mathcal{O},\\mathcal{T},\\mathcal{Z},r,\\rho,\\gamma) following conventions similar to prior work [16, 17, 18]. We are given an instruction u‚ààùí∞u\\in\\mathcal{U}, user action preferences p=(0‚â§pa‚â§1‚Äã for ‚Äãa‚ààùíú)p=\\left(0\\leq p_{a}\\leq 1\\text{ for }a\\in\\mathcal{A}\\right), an initial state drawn from œÅ(‚ãÖ|u)\\rho(\\cdot\\,|\\,u), an initial observation o0‚ààùí™o_{0}\\in\\mathcal{O}, and the environment state space ùíÆ\\mathcal{S}. At step kk, the Orchestrator chooses an action ak‚ààùíúa_{k}\\in\\mathcal{A} according to a policy œÄŒ∏‚Äã(ak|hk)\\pi_{\\theta}(a_{k}\\,|\\,h_{k}) where hk=(u,o0,a0,o1,‚Ä¶,ak‚àí1,ok)h_{k}=(u,o_{0},a_{0},o_{1},\\dots,a_{k-1},o_{k}) is the interaction history. The environment transitions according to ùíØ‚Äã(sk+1|sk,ak)\\mathcal{T}(s_{k+1}\\,|\\,s_{k},a_{k}) and emits an observation ok+1‚àºùíµ(‚ãÖ|sk+1,ak)o_{k+1}\\sim\\mathcal{Z}(\\cdot\\,|\\,s_{k+1},a_{k}). The actions aia_{i} come at costs cic_{i} and operational latency lil_{i}, and the alignment of each action with user preferences is paip_{a_{i}}. After NN interaction steps, Orchestrator has traced the trajectory œÑ=hN\\tau=h_{N} and the environment provides a reward r‚Äã(œÑ)‚àà[0,1]r(\\tau)\\in[0,1] based on its correctness. Our goal is to maximize the correctness reward r‚Äã(œÑ)r(\\tau) and the overall user preference alignment ‚àëpai\\sum p_{a_{i}} while minimizing the total cost ‚àëci\\sum c_{i} and the aggregate latency ‚àëli\\sum l_{i}.\n\n### 2.2 Multi-Turn Rollout\n\nGiven a user task, Orchestrator produces a solution via an iterative rollout that interleaves tool use with environment feedback to form a trajectory of turns. The rollout is initialized with a predefined system prompt and the question; the model (assistant role) then generates an initial step that ends with an EOS token. Each turn follows a reasoning‚Äìaction‚Äìobservation loop: (1) Chain-of-thought (reasoning). The Orchestrator analyzes the current state and plans the next action. (2) Tool call (action). Based on its reasoning, Orchestrator selects a tool from the available set (e.g., APIs, specialized models, code interpreters) and specifies parameters. (3) Tool response (observation). If a tool call is present, the tool-call block is extracted and executed by the environment; the resulting output is appended to the context under the user role and fed back to the model for the next turn. This process repeats until Orchestrator receives a termination signal from the environment or the rollout reaches a maximum of 50 turns.\n\n## 3 ToolOrchestra\n\nOur approach, ToolOrchestra, centers on training a small language model as an intelligent agentic model capable of solving complex tasks by dynamically selecting and utilizing a wide variety of external tools. We hypothesize that small language models suffice for this purpose if they are taught to coordinate more intelligent tools strategically, and thus choose to train an 8B model. ToolOrchestra consists of an end-to-end reinforcement learning setup where the model under training, termed Orchestrator, learns to generate optimal multi-step reasoning and tool-use trajectories. The overall architecture is illustrated in Figure 2.\n\n### 3.1 Unified Tool Calling\n\nIn contrast to prior tool-use agents [19, 20], we broaden the toolset to include domain-specialized models and expose all tools through a single, unified interface. Tools are specified in JSON as a list of objects; each object defines the tool name, description, and a typed parameter schema (names and descriptions). When LLMs are used as tools, we obtain their descriptions with the following steps: (1). randomly sample 10 training tasks; (2). obtain the trajectories of LLMs to finish these tasks; (3). Ask another LLM to write the description based on the task instructions, LLM trajectories and whether LLMs complete the tasks. In Appendix C, we show an example description of Qwen3-32B. The complete catalog of tools used in our training is provided in Appendix D.\n\n### 3.2 End-to-End Agentic Reinforcement Learning\n\n### 3.3 Data Synthesis\n\n## 4 Experimental Setting\n\n### 4.1 Tools\n\nIn the training, we prepare a large and comprehensive tool set (Appendix D), but only sample a subset for each training instance to build diverse tool configurations (¬ß3.3). We fix the following tool set in the evaluation for fair comparison.\n\nBasic tools. We use Tavily search API 111https://www.tavily.com/ for web search, Python sandbox for Code interpreter, and build Faiss index with Qwen3-Embedding-8B [22] for local search. Additionally, we also incorporate domain-specific functions, such as get_flight_status, to address specialized challenges within those domains.\n\nSpecialized LLMs. We prompt GPT-5 [23], GPT-5-mini [23] as code writer, employ Qwen2.5-Coder-32B-Instruct [24] as another code writer, and leverage Qwen2.5-Math-72B [25], Qwen2.5-Math-7B [25] as specialized math models.\n\nGeneralist LLMs. We consider GPT-5, GPT-5-mini, Llama-3.3-70B-Instruct [26], and Qwen3-32B [27] as representative generalist models.\n\n### 4.2 Baselines\n\nWe compare Orchestrator-8B produced by ToolOrchestra to baseline orchestrators constructed by prompting LLMs. Additionally, we also compare to off-the-shelf monolithic LLM systems that are (1) not equipped with tools, (2) equipped with basic tools, and (3) using the expanded tool set that further includes specialized expert models and strong generalist models.\n\nFor off-the-shelf LLMs, we evaluate GPT-5, Claude Opus 4.1 [28], Llama-3.3-70B-Instruct, Qwen3-235B-A22B [27], Llama-3_3-Nemotron-Super-49B-v1 [29], Qwen3-8B [27].\n\n### 4.3 Evaluation Configuration\n\nWe conduct experiments on three popular benchmarks with complex reasoning: Humanity‚Äôs Last Exam (HLE), FRAMES, and œÑ2\\tau^{2}-Bench. Details about these three benchmarks are given in Appendix B. Throughout the evaluation, we use the official price for proprietary models and leverage the pricing systems of TogetherAI222https://www.together.ai/pricing for open-source models. We set the inference temperature to 0 and allow maximum 50 turn for Orchestrator to solve a task.\n\n### 4.4 Training Configuration\n\nWe employ Qwen3-8B as the backbone LLM and train it on the GeneralThought-430K 333https://huggingface.co/datasets/natolambert/GeneralThought-430K-filtered dataset in conjunction with synthetic data (¬ß\\S3.3). The training configuration uses a learning rate of 1e-6, a maximum input sequence length of 24,000, and a maximum generation length of 8,000, with a training batch size of 16 and a rollout batch size of 8. We allow maximum 50 turns for the Orchestrator to finish a task during rollout and use 16 NVIDIA H100 GPUs throughout the training.\n\n## 5 Experimental Results\n\nWe compare Orchestrator against a wide range of baselines across HLE, FRAMES, and œÑ2\\tau^{2}-Bench. The results are summarized in Table 1. For simple prompting methods without tools, models such as Qwen3-235B-A22B and Llama-3.3-70B fail to demonstrate strong performance. This highlights the inherent difficulty of the benchmarks, where tool use or additional reasoning mechanisms is essential. Providing tool access improves performance in some cases. For instance, Claude Opus 4.1 with tool usage improves from 11.7 to 19.8 in HLE, and from 58.2 to 63.5 in FRAMES, but at the expense of 2.8x costs and 4x latency. Smaller models like Qwen3-8B perform poorly (4.7 on HLE), indicating that basic tools alone are insufficient. Combining tools with specialized and generalist LLMs generally improves results ‚Äî Qwen3-235B-A22B, for example, rises from 14.0 to 32.8 on HLE and from 39.5 to 74.2 on FRAMES, but consumes more than 2 times of cost and latency. However, the gains are inconsistent across different models. GPT-5 using both tools and models suffers from performance drop due to inherent biases, often defaulting to GPT-5-mini (¬ß6.1).\n\nIn contrast, our Orchestrator-8B achieves 37.1 on HLE and 76.3 on FRAMES, surpassing all baselines by a large margin. In œÑ2\\tau^{2}-Bench, Orchestrator-8B outperforms GPT-5 using basic tools by 2.5%, exhibiting strong function calling capabilities. Notably, compared to GPT-5 with tool use (35.1 on HLE) and Qwen3-235B-A22B with tool + model (32.8 on HLE), our approach delivers consistent improvements of +2 to +4.3 absolute points, while using only a small fraction of cost and time. These gains are particularly striking given that Orchestrator has only 8B parameters, but is capable of coordinating more intelligent models such as GPT-5, and achieves better performance with lower cost, which highlights the effectiveness of the orchestration strategy. Overall, the results clearly demonstrate the effectiveness of ToolOrchestra and the superiority of Orchestrator model in both performance and efficiency.\n\n## 6 Analysis\n\n### 6.1 Tool Use Analysis\n\nFigure 5 shows the proportion of calls to each tool when various models serve as the orchestrator to solve a task. Instead of excessively invoking strong models and expensive tools, Orchestrator-8B learns to coordinate them more strategically. For example, in choosing between different models, Claude Opus 4.1 relies on GPT-5 most of the time, while making fewer calls to other models. In contrast, GPT-5 prefers to use GPT-5-mini. Orchestrator-8B learns to choose between various tools strategically, and achieves superior performance while using significantly lower costs.\n\n### 6.2 Cost Analysis\n\nTo analyze the cost-effectiveness, we display the performance on HLE as a function of cost in Figure 6. We experiment with settings where the maximum number of 10, 20, 50 and 100 turns are allowed to finish a task. As the maximum number of allowed turns increases (i.e., cost increases), all models show improved performance. Orchestrator-8B consistently outperforms GPT-5, Claude Opus 4.1 and Qwen3-235B-A22B at a given budget, and can achieve similar results at a substantially lower cost. This demonstrates the capability of Orchestrator-8B to manage a heterogeneous set of tools, and pushes the intelligence boundary of the system as a whole.\n\n### 6.3 Generalization\n\nTo evaluate Orchestrator-8B‚Äôs generalization capability, we test it with a tool configuration containing models unseen during training: (1) Query writer: Claude Opus 4.1, o3-mini and GPT-4o [30]; (2) Code writer: Claude Opus 4.1, Claude Sonnet 4.1 and Codestral-22B-v0.1 [31]; (3) Math model: OpenMath-Llama-2-70b [32], DeepSeek-Math-7b-Instruct [21]; (4) Generalist Models: Claude Opus 4.1, Claude Sonnet 4.1 and Gemma-3-27b-it [33].\n\nWe keep the basic tools (web search, local search and code interpreter) as the same mentioned in ¬ß4.1 and generate model descriptions following the same procedures mentioned in section ¬ß3.1. Table 2 demonstrates that Orchestrator-8B shows strong skills in using models as tools. Even provided with a set of models not seen during training, Orchestrator successfully adapts to it by understanding their skills and weaknesses from model descriptions, and consistently achieves the best performance at the lowest cost across HLE, Frames and œÑ2\\tau^{2}-Bench. This confirms that the diverse tool configurations during training effectively enhance the generalization capabilities of Orchestrator-8B. In Appendix H, we conduct further experiments to evaluate Orchestrator-8B on pricing configurations unseen in training.\n\n### 6.4 User Preferences\n\nTo assess Orchestrator-8B‚Äôs ability to adapt to heterogeneous user preferences at test time, we evaluate it on the Preference-aware test set described in ¬ß3.3, where we concatenate each question with an additional preference instruction. We evaluate the model preference adherence performance by calculating the preference-aware rewards defined in Appendix L. Table 3 shows that, even strong monolithic systems such as GPT-5 struggle to faithfully follow user preferences. In contrast, Orchestrator-8B exhibits remarkably better adherence to user preferences.\n\n## 7 Related Work\n\n### 7.1 From Tool Learning to Generalist Agents\n\nTool learning underpins advanced reasoning in LLMs, as many tasks require external APIs, search engines, or code interpreters. Early work [3, 2, 6] used supervised fine-tuning (SFT) on tool-labeled data like GPT-4 generated dialogues. More recently, tool use has been framed as a sequential decision-making problem optimized by RL, with systems such as WebGPT [34], Search-R1 [20], ToRL [19], StepTool [7], SWiRL [8], Nemotron-Research-Tool-N1 [9], and ToolRL [10]. Building on this foundation, generalist agents like deep research agents [35, 36, 37, 38] autonomously discover, analyze, and synthesize across sources to produce analyst-level reports, aligning with the vision of compound AI systems [39, 40]. Recent open-source frameworks like SmolAgent [41], WebAgent [42, 43, 44], OWL [45], AutoAgent [46], and OAgent [47] extend this trend toward modular, robust, and accessible systems, highlighting the broader democratization of generalist agents.\n\n### 7.2 From Tool-Use Accuracy to Efficiency and Controllability\n\nBeyond correctness, recent work emphasizes efficiency and controllability, aiming to reduce computational costs and better align outputs with user preferences. Prompting-based methods like Self Divide-and-Conquer [48], Efficient Agents [49], and SMART [50] adaptively invoke tools or fine-tune usage, though they often depend on heavy prompt engineering or curated datasets. RL provides a more flexible alternative, where reward shaping balances accuracy, efficiency, and reliability. Advances include integrating auxiliary signals (e.g., response length, task difficulty)[51, 52, 53] and combining verifiable signals with human feedback[54]. A related direction is weak-to-strong generalization [55], which explores eliciting stronger models from weaker supervision. The most relevant work, OTC [56], improves efficiency by penalizing excessive calls while preserving accuracy. Unlike the prior work, our approach addresses the broader orchestration problem by using RL to coordinate diverse tools and models, balancing correctness, efficiency, and user preference for finer alignment and more robust deployment.\n\n## 8 Conclusion\n\nIn this work, we presented ToolOrchestra, a method for training a small orchestration model to unify diverse tools and specialized models. By training Orchestrator end-to-end with reinforcement learning, we showed that it can learn to plan adaptive tool-use strategies guided by both outcome quality, efficiency, and human preference rewards. This enables the agent to dynamically balance performance and cost, rather than relying on static heuristics or purely supervised approaches. To aid reinforcement learning, we also contribute a complex user-agent-tool synthetic dataset ToolScale. Our experiments on challenging benchmarks demonstrate that our Orchestrator-8B attains state-of-the-art performance while operating at significantly lower cost compared to larger models. Looking ahead, we envision more sophisticated recursive orchestrator systems to push the upper bound of intelligence but also to further enhance efficiency in solving increasingly complex agentic tasks.\n",
    "extracted_at": "2025-12-26 16:40:27.943881"
  },
  "classification": {
    "paper_id": "2511.21689",
    "category": "generative_models",
    "category_name": "Generative Models",
    "category_name_zh": "ÁîüÊàêÊ®°Âûã",
    "confidence": 0.999,
    "reasoning": "The paper discusses training small orchestrators that coordinate models and tools, emphasizing efficiency and intelligence. This aligns with generative models as they focus on data generation and model orchestration.",
    "raw_response": "{\n  \"category\": \"generative_models\",\n  \"confidence\": 0.999,\n  \"reasoning\": \"The paper discusses training small orchestrators that coordinate models and tools, emphasizing efficiency and intelligence. This aligns with generative models as they focus on data generation and model orchestration.\"\n}",
    "generated_at": "2025-12-26 18:18:08.137472"
  },
  "keywords": {
    "paper_id": "2511.21689",
    "keywords": [
      "ToolOrchestra",
      "model",
      "orchestration",
      "efficient model",
      "agentic tasks",
      "orchestrators",
      "complex problems",
      "intelligence"
    ],
    "keywords_zh": [
      "Â∑•ÂÖ∑ÁªºÂêà‰Ωì",
      "Ê®°Âûã",
      "ÂçèË∞ÉÂô®",
      "È´òÊïàÊ®°Âûã",
      "Â§çÊùÇ‰ªªÂä°",
      "Êô∫ËÉΩ‰Ωì",
      "ÂçèÂêåÂ∑•‰Ωú"
    ],
    "raw_response": "{\n  \"keywords\": [\n    \"ToolOrchestra\",\n    \"model\",\n    \"orchestration\",\n    \"efficient model\",\n    \"agentic tasks\",\n    \"orchestrators\",\n    \"complex problems\",\n    \"intelligence\"\n  ],\n  \"keywords_zh\": [\n    \"Â∑•ÂÖ∑ÁªºÂêà‰Ωì\",\n    \"Ê®°Âûã\",\n    \"ÂçèË∞ÉÂô®\",\n    \"È´òÊïàÊ®°Âûã\",\n    \"Â§çÊùÇ‰ªªÂä°\",\n    \"Êô∫ËÉΩ‰Ωì\",\n    \"ÂçèÂêåÂ∑•‰Ωú\"\n  ]\n}",
    "generated_at": "2025-12-26 18:18:10.901922"
  },
  "labels": {
    "paper_id": "2511.21689",
    "labels": [
      "model orchestration",
      "AI",
      "machine learning",
      "efficiency",
      "intelligence"
    ],
    "labels_zh": [
      "Ê®°ÂûãÂçèÂêå",
      "‰∫∫Â∑•Êô∫ËÉΩ",
      "Êú∫Âô®Â≠¶‰π†",
      "ÊïàÁéáÊèêÂçá",
      "Êô∫ËÉΩ"
    ],
    "raw_response": "{\n  \"labels\": [\n    \"model orchestration\",\n    \"AI\",\n    \"machine learning\",\n    \"efficiency\",\n    \"intelligence\"\n  ],\n  \"labels_zh\": [\n    \"Ê®°ÂûãÂçèÂêå\",\n    \"‰∫∫Â∑•Êô∫ËÉΩ\",\n    \"Êú∫Âô®Â≠¶‰π†\",\n    \"ÊïàÁéáÊèêÂçá\",\n    \"Êô∫ËÉΩ\"\n  ]\n}",
    "generated_at": "2025-12-26 18:18:13.870596"
  },
  "comments": null,
  "notion_page_id": null,
  "notion_synced_at": null,
  "created_at": "2025-12-26 18:22:45.445187",
  "updated_at": "2025-12-26 18:22:45.445190"
}